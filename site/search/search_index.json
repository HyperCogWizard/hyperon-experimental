{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"CONTRIBUTING/","title":"How to contribute","text":""},{"location":"CONTRIBUTING/#making-changes","title":"Making changes","text":"<p>Before making changes create a personal fork of the repository. Sync fork and create new branch from the latest version of the <code>main</code> branch. Create separate branch for each change. Thus it is simpler to support consistent state of the <code>main</code> in your fork.</p> <p>Prefer incremental commits to one big commit which contains the whole change. Prefer commits which passes all of the tests. If it is not possible to satisfy both requirements at once you can make single commit which passes tests or mark tests as ignored until change is done.</p> <p>Each commit should have a message in the following format: <pre><code>Change summary in 50 characters (less than 73 characters)\n\nOptional detailed description of the change when required. Each line\nis less than 73 character long.\n</code></pre> Such commits looks better in GitHub history.</p> <p>Please avoid generic commit messages like <code>Update README.md</code>. Good commit message should describe the change, not a fact of the change. For example <code>Add troubleshooting section about No module named 'hyperonpy'</code>. By looking to the commit message history the reviewer should understand the order and brief description of changes.</p> <p>Please don't include number and description of the issue into a commit summary line. Use <code>Fixes #&lt;issue-number&gt;</code> in the pull request description instead to link the PR and the issue.</p> <p>PR should satisfy the following requirement before being merged: - contain latest changes from the repo; - pass tests; - be reviewed.</p> <p>Feel free to raise draft PR if you need an advice or help with your changes.</p>"},{"location":"CONTRIBUTING/#code-style","title":"Code style","text":"<p>We have small set of code style rules for now. The rule of thumb is to take a look at the existing code and stick to its style.</p>"},{"location":"CONTRIBUTING/#general","title":"General","text":"<p>If you want to leave some reminder in code, for example to fix something later, you can do it by two ways. Add a comment starting with <code>FIXME</code> to mark something which should be done before the PR is merged. Add a comment starting with <code>TODO</code> to mark the improvement which can be postponed and done later by a separate PR. The main purpose of a <code>TODO</code> comment is to trigger a developer who looks at the code after you and make him fix the issue if it is appropriate. If the change or question is big enough or it affects the API of the module it is better to raise an issue instead.</p>"},{"location":"CONTRIBUTING/#libraries","title":"Libraries","text":"<p>When adding new library into the project please ensure you specify the exact version instead of using ranges. The minor update of the library can break the build unexpectedly. The broken build is a real burden because most of the users build the project from the source.</p>"},{"location":"CONTRIBUTING/#rust","title":"Rust","text":"<p>When working on Rust C API prefer making <code>unsafe</code> blocks as small as possible. This makes it easier to find blocks that might be source of issues. Usually it is not required to mark C API functions <code>unsafe</code> because they are not intended to be used from the Rust safe code.</p>"},{"location":"CONTRIBUTING/#git","title":"Git","text":"<p><code>.gitignore</code> file is also used to exclued files going into Docker image building context. All ignored files should be put into the same <code>.gitignore</code> at the root of the repository by this reason.</p>"},{"location":"DEVELOPMENT/","title":"Instructions for developers","text":""},{"location":"DEVELOPMENT/#how-to-release-python-distribution-packages-locally","title":"How to release Python distribution packages locally","text":"<p>Python packages are released using cibuildwheel. First step is to setup it. Usually it means setup docker and install the package from PyPi (see setup instructions).</p> <p>There are additional preparations to be made. First of all it is needed to allow building and installing <code>libhyperonc</code> library on a build environment. <code>cibuildwheel</code> uses isolated docker container for each kind of platform it supports. Only code of the Python package is copied into container automatically. Code of the <code>libhyperonc</code> library should be downloaded from outside. It means one need to have the code in some repo accessible from the container before starting release. The simplest way is to push the changes in your GitHub repo fork.</p> <p>By default library downloads and installs version from the <code>main</code> branch of the <code>trueagi-io/hyperon-experimental</code> repository. Using a custom branch is done by passing custom parameters to the <code>install-hyperonc.sh</code> script through CIBW_BEFORE_ALL environment variable: <pre><code>export CIBW_BEFORE_ALL='sh -c \"./python/install-hyperonc.sh -u &lt;git-repo-url&gt; -r &lt;git-branch&gt;\"'\n</code></pre> One should replace <code>&lt;git-repo-url&gt;</code> and <code>&lt;git-branch&gt;</code> by the repo URL and branch which are used in release.</p> <p>Also it is possible to start from building the only platform to quickly check whether release works. This can be done using CIBW_BUILD variable: <pre><code>export CIBW_BUILD=cp310-manylinux_x86_64\n</code></pre></p> <p>After exporting the variables above one can start release by executing <code>cibuildwheel ./python</code> from the root directory of the repo. See cibuildwheel documentation for details.</p>"},{"location":"DEVELOPMENT/#how-to-update-the-version","title":"How to update the version","text":"<p>Usually it is needed before releasing the artifacts or before making a test release.</p> <p>There are three locations to update: - /Cargo.toml file:   - <code>workspace.package.version</code> property   - <code>workspace.dependencies.hyperon.version</code> property - /python/VERSION file</p> <p>All three locations should contain the same version.</p>"},{"location":"DEVELOPMENT/#how-to-release-binaries","title":"How to release binaries","text":"<p>Update the version How to update the version in the main branch of the repository, raise PR and merge it. Use Create a new release link on the main page of the GitHub repo. Press <code>Choose a tag</code> control and type new tag which should be in form of <code>v&lt;version&gt;</code> (for example if version is <code>0.1.7</code> then tag is <code>v0.1.7</code>). After typing the tag press <code>Create new tag on publish</code>. Now press <code>Generate release notes</code> button. It will automatically fill the <code>Release title</code> and <code>Release description</code> fields. Tick <code>Set as a pre-release</code> checkbox if needed and press <code>Publish release</code> button.  Now you have published new GitHub release and triggered a job to build release artifacts.</p> <p>After release job is finished one need to approve publishing artifacts to the PyPi repository. Before approving one can download and test Python wheels built. To check the job status go the <code>Actions/release-pyhon</code> page, and select last workflow run. Links to the archives with the artifacts are located at the bottom of the page.</p> <p>If distribution wheels are good then one can approve the publishing process. At the top of the workflow run page there are two blocks <code>Publish to Test PyPi</code> and <code>Publish to PyPi</code>. First press <code>Publish to Test PyPi</code> block approve it and wait for publishing. It is critical to start from Test PyPi because release cannot be removed from the PyPi after publishing.</p> <p>After release is published check it can be installed executing: <pre><code>python3 -m pip install --index-url https://test.pypi.org/simple/ hyperon\n</code></pre> Check that the latest published version was downloaded and installed. If you are making a test release then you should not publish it to the PyPi. If it is a production release then proceed with <code>Publish to PyPi</code> block.</p>"},{"location":"DEVELOPMENT/#how-to-check-release-job-in-fork","title":"How to check release job in fork","text":"<p>First you need to select the test release version. It should contain an additional version digit after the latest officially released version. Let's say the latest released version is <code>0.1.7</code>. Then the test release version should be <code>0.1.7.x</code> for instance <code>0.1.7.1</code>. Start from 1 and increment it after each release you published successfully.</p> <p>Make a separate branch to release the code. It is not necessary but it is highly recommended to not pollute the main branch of the fork. In order to be able releasing from the branch one need to temporary make it default branch. It is done by using GitHub repo <code>Settings/General/Default branch</code> control.</p> <p>Update the version in the branch to the test release version you constructed. Commit and push this change in your test branch. Now you are ready to make a test release. See release binaries instruction.</p> <p>After testing the release procedure remove the commit with version update from your branch. And set default branch setting to the previous value.</p>"},{"location":"architecture/","title":"Hyperon System Architecture","text":"<p>This documentation presents the architectural overview of the OpenCog Hyperon system, illustrating the recursive and emergent nature of its design patterns and the neural-symbolic integration mechanisms.</p>"},{"location":"architecture/#high-level-system-overview","title":"High-Level System Overview","text":"<p>The Hyperon system is built around a modular, cognitive architecture that integrates symbolic reasoning with adaptive attention allocation mechanisms. The core architecture follows recursive design patterns that enable emergent cognitive behavior.</p> <pre><code>graph TD\n    A[Environment] --&gt; B[Metta Runner]\n    B --&gt; C[Module System]\n    B --&gt; D[Space System]\n    B --&gt; E[Tokenizer System]\n\n    C --&gt; F[MettaMod]\n    C --&gt; G[ModuleLoader]\n    C --&gt; H[Package Management]\n\n    D --&gt; I[GroundingSpace]\n    D --&gt; J[ModuleSpace]\n    D --&gt; K[AtomSpace]\n\n    E --&gt; L[Context-Independent Tokens]\n    E --&gt; M[Context-Dependent Tokens]\n    E --&gt; N[Standard Library Tokens]\n\n    F --&gt; O[Imported Dependencies]\n    F --&gt; P[Resource Directory]\n    F --&gt; Q[Module Tokenizer]\n\n    B --&gt; R[RunnerState]\n    R --&gt; S[RunContext]\n    S --&gt; T[Interpreter State]\n\n    style A fill:#e1f5fe\n    style B fill:#f3e5f5\n    style C fill:#e8f5e8\n    style D fill:#fff3e0\n    style E fill:#fce4ec</code></pre>"},{"location":"architecture/#key-architectural-components","title":"Key Architectural Components","text":"<ul> <li>Environment: Platform abstraction layer managing configuration, security, and external resources</li> <li>Metta Runner: Core execution engine hosting all state and coordinating module execution</li> <li>Module System: Hierarchical module management with dependency resolution and namespace isolation</li> <li>Space System: Multi-layered atom storage and retrieval system with cognitive space abstractions</li> <li>Tokenizer System: Adaptive token registration and parsing with context-aware resolution</li> </ul>"},{"location":"architecture/#module-system-architecture","title":"Module System Architecture","text":"<p>The module system implements a sophisticated hierarchical structure with recursive loading patterns and adaptive namespace management.</p> <pre><code>graph LR\n    A[Metta Runner] --&gt; B[Module Names Tree]\n    A --&gt; C[Module Vector]\n    A --&gt; D[Module Descriptors]\n\n    B --&gt; E[Top Module]\n    E --&gt; F[Sub-Module A]\n    E --&gt; G[Sub-Module B]\n    F --&gt; H[Deep Sub-Module]\n\n    C --&gt; I[\"ModId(0) - Top\"]\n    C --&gt; J[\"ModId(1) - CoreLib\"]\n    C --&gt; K[\"ModId(2) - StdLib\"]\n    C --&gt; L[\"ModId(3+) - User Modules\"]\n\n    I --&gt; M[MettaMod Instance]\n    M --&gt; N[Space]\n    M --&gt; O[Tokenizer]\n    M --&gt; P[Imported Dependencies]\n    M --&gt; Q[Resource Directory]\n\n    style A fill:#e1f5fe\n    style B fill:#e8f5e8\n    style C fill:#fff3e0\n    style D fill:#fce4ec\n    style M fill:#f3e5f5</code></pre>"},{"location":"architecture/#module-lifecycle-and-initialization","title":"Module Lifecycle and Initialization","text":"<pre><code>sequenceDiagram\n    participant E as Environment\n    participant M as Metta Runner\n    participant MS as ModuleInitState\n    participant RS as RunnerState\n    participant RC as RunContext\n    participant ML as ModuleLoader\n\n    E-&gt;&gt;M: new_with_stdlib_loader()\n    M-&gt;&gt;M: load corelib module\n    M-&gt;&gt;M: load stdlib module\n    M-&gt;&gt;M: import builtin modules\n\n    M-&gt;&gt;RS: new_for_loading()\n    RS-&gt;&gt;MS: init_module()\n    MS-&gt;&gt;ML: load(context)\n    ML-&gt;&gt;RC: init_self_module()\n    RC-&gt;&gt;RC: setup space &amp; tokenizer\n    ML-&gt;&gt;RC: push_parser(MeTTa code)\n    RS-&gt;&gt;RS: run_to_completion()\n    RS-&gt;&gt;M: finalize_loading()\n    M-&gt;&gt;M: merge_init_state()</code></pre>"},{"location":"architecture/#neural-symbolic-integration-patterns","title":"Neural-Symbolic Integration Patterns","text":"<p>The system implements neural-symbolic integration through adaptive attention allocation and cognitive synergy optimization mechanisms.</p> <pre><code>stateDiagram-v2\n    [*] --&gt; ADD: Parse Input\n    ADD --&gt; INTERPRET: Execute Symbol (!)\n    INTERPRET --&gt; Processing: Start Interpretation\n    Processing --&gt; TypeCheck: Type Validation\n    TypeCheck --&gt; SymbolicReasoning: Valid Types\n    TypeCheck --&gt; ErrorState: Type Mismatch\n    SymbolicReasoning --&gt; AdaptiveAttention: Apply Reasoning\n    AdaptiveAttention --&gt; CognitiveIntegration: Attention Allocation\n    CognitiveIntegration --&gt; ResultGeneration: Neural-Symbolic Synthesis\n    ResultGeneration --&gt; ADD: Continue Processing\n    ResultGeneration --&gt; [*]: Complete\n    ErrorState --&gt; [*]: Terminate\n\n    note right of AdaptiveAttention\n        Recursive attention patterns\n        - Stack depth management\n        - Context-aware processing\n        - Emergent focus allocation\n    end note\n\n    note right of CognitiveIntegration\n        Neural-symbolic synergy\n        - Pattern recognition\n        - Symbolic manipulation\n        - Adaptive learning\n    end note</code></pre>"},{"location":"architecture/#data-flow-and-execution-patterns","title":"Data Flow and Execution Patterns","text":"<p>The execution model follows recursive patterns with emergent data flows across cognitive subsystems.</p> <pre><code>graph TD\n    A[Input Parser] --&gt; B[Tokenizer Resolution]\n    B --&gt; C{Token Type}\n\n    C --&gt;|Symbol| D[Atom Construction]\n    C --&gt;|Operator| E[Function Resolution]\n    C --&gt;|Literal| F[Direct Value]\n\n    D --&gt; G[Space Query/Add]\n    E --&gt; H[Execution Context]\n    F --&gt; G\n\n    G --&gt; I{Operation Mode}\n    I --&gt;|ADD| J[Add to Space]\n    I --&gt;|INTERPRET| K[Cognitive Processing]\n\n    J --&gt; L[Type Validation]\n    K --&gt; M[Interpreter State]\n\n    L --&gt; N{Valid?}\n    N --&gt;|Yes| O[Success]\n    N --&gt;|No| P[Error Generation]\n\n    M --&gt; Q[Recursive Evaluation]\n    Q --&gt; R[Pattern Matching]\n    R --&gt; S[Result Synthesis]\n    S --&gt; O\n\n    O --&gt; T[Output Results]\n    P --&gt; U[Error Propagation]\n    U --&gt; T\n\n    style G fill:#e8f5e8\n    style M fill:#f3e5f5\n    style Q fill:#fff3e0\n    style R fill:#fce4ec</code></pre>"},{"location":"architecture/#recursive-implementation-pathways","title":"Recursive Implementation Pathways","text":"<p>The system exhibits recursive patterns at multiple levels, enabling emergent cognitive behavior through self-similar structures.</p> <pre><code>graph TB\n    subgraph \"Recursive Module Loading\"\n        A1[Parent Module Request] --&gt; B1[Dependency Resolution]\n        B1 --&gt; C1[Child Module Loading]\n        C1 --&gt; D1[Recursive Dependency Check]\n        D1 --&gt; C1\n        D1 --&gt; E1[Module Integration]\n    end\n\n    subgraph \"Recursive Space Querying\"\n        A2[Query Pattern] --&gt; B2[Local Space Search]\n        B2 --&gt; C2[Dependency Space Search]\n        C2 --&gt; D2[Recursive Space Traversal]\n        D2 --&gt; C2\n        D2 --&gt; E2[Result Aggregation]\n    end\n\n    subgraph \"Recursive Interpretation\"\n        A3[Expression Input] --&gt; B3[Parse Structure]\n        B3 --&gt; C3[Sub-expression Eval]\n        C3 --&gt; D3[Recursive Descent]\n        D3 --&gt; C3\n        D3 --&gt; E3[Result Composition]\n    end\n\n    E1 --&gt; F[Emergent System Behavior]\n    E2 --&gt; F\n    E3 --&gt; F\n    F --&gt; G[Adaptive Cognitive Patterns]\n\n    style A1 fill:#e1f5fe\n    style A2 fill:#e8f5e8\n    style A3 fill:#fff3e0\n    style F fill:#f3e5f5\n    style G fill:#fce4ec</code></pre>"},{"location":"architecture/#cognitive-attention-allocation-mechanisms","title":"Cognitive Attention Allocation Mechanisms","text":"<p>The system implements adaptive attention allocation through hierarchical processing and emergent focus patterns.</p> <pre><code>flowchart TD\n    A[Attention Entry Point] --&gt; B{Processing Priority}\n\n    B --&gt;|High| C[Immediate Processing]\n    B --&gt;|Medium| D[Queued Processing]\n    B --&gt;|Low| E[Background Processing]\n\n    C --&gt; F[Context Allocation]\n    D --&gt; G[Priority Queue Management]\n    E --&gt; H[Resource Pool]\n\n    F --&gt; I[Cognitive Resource Assignment]\n    G --&gt; J[Attention Scheduling]\n    H --&gt; K[Background Task Pool]\n\n    I --&gt; L[Active Processing Context]\n    J --&gt; M[Deferred Execution Queue]\n    K --&gt; N[Passive Resource Monitoring]\n\n    L --&gt; O[Recursive Attention Patterns]\n    M --&gt; P[Attention Flow Management]\n    N --&gt; Q[Resource Optimization]\n\n    O --&gt; R[Emergent Focus Allocation]\n    P --&gt; R\n    Q --&gt; R\n\n    R --&gt; S[Adaptive Cognitive Response]\n\n    style A fill:#e1f5fe\n    style I fill:#e8f5e8\n    style O fill:#fff3e0\n    style R fill:#f3e5f5\n    style S fill:#fce4ec</code></pre>"},{"location":"architecture/#system-integration-and-extension-points","title":"System Integration and Extension Points","text":"<p>The architecture provides multiple extension points for cognitive enhancement and neural-symbolic integration.</p> <pre><code>mindmap\n  root((Hyperon Core))\n    Language Bindings\n      Rust Native\n      C/C++ FFI\n      Python Integration\n      Future Languages\n    Module Extensions\n      Standard Library\n      Domain-Specific Modules\n      Cognitive Extensions\n      Neural Network Integration\n    Space Extensions\n      Custom Space Types\n      Distributed Spaces\n      Persistent Storage\n      Cognitive Memory Models\n    Cognitive Patterns\n      Attention Mechanisms\n      Learning Algorithms\n      Reasoning Engines\n      Pattern Recognition\n    Integration Points\n      External Systems\n      Database Connections\n      Network Services\n      Hardware Acceleration</code></pre> <p>This architecture documentation captures the recursive and emergent nature of the Hyperon system, providing the foundation for understanding its neural-symbolic cognitive patterns and adaptive attention allocation mechanisms.</p>"},{"location":"building_c_docs/","title":"Building c docs","text":""},{"location":"building_c_docs/#building-c-documentation","title":"Building C Documentation","text":"<ol> <li> <p><code>doxygen</code> must be installed.  Depending on your platform it may be easiest to use a package manager.</p> <ul> <li>using apt: <code>sudo apt-get install doxygen</code></li> <li>using homebrew (Mac): <code>brew install doxygen</code></li> <li>installing from source: [https://www.doxygen.nl/manual/install.html]</li> </ul> </li> <li> <p>Build the Hyperon C library, following the instructions here: [https://github.com/trueagi-io/hyperon-experimental?tab=readme-ov-file#c-and-python-api]</p> </li> <li> <p>Set CMake environment variables.  Relative to the <code>build</code> directory, run the following:</p> <ul> <li><code>export CMAKE_CURRENT_SOURCE_DIR=../c/</code></li> <li><code>export HYPERONC_INCLUDE_DIR=./hyperonc-install/include/hyperonc/hyperon/</code></li> </ul> </li> <li> <p>Invoke <code>doxygen</code> using the following command: <code>doxygen ../c/hyperonc.doxyfile</code></p> </li> </ol> <p>The top page for the rendered HTML results will be written to <code>./html/index.html</code>, and latex results will be similarly written to <code>./latex/index.tex</code></p>"},{"location":"cognitive-patterns/","title":"Cognitive Patterns and Neural-Symbolic Integration","text":"<p>The Hyperon system implements sophisticated cognitive patterns through neural-symbolic integration, enabling emergent intelligence, adaptive learning, and recursive self-improvement capabilities.</p>"},{"location":"cognitive-patterns/#cognitive-architecture-overview","title":"Cognitive Architecture Overview","text":"<p>The cognitive architecture implements a multi-layered approach to intelligence that integrates symbolic reasoning with neural processing through adaptive attention mechanisms.</p> <pre><code>graph TB\n    subgraph \"Meta-Cognitive Layer\"\n        A1[Self-Awareness] --&gt; A2[Meta-Learning]\n        A2 --&gt; A3[Cognitive Monitoring]\n        A3 --&gt; A4[Adaptive Control]\n    end\n\n    subgraph \"Abstract Reasoning Layer\" \n        B1[Logical Inference] --&gt; B2[Pattern Abstraction]\n        B2 --&gt; B3[Conceptual Integration]\n        B3 --&gt; B4[Knowledge Synthesis]\n    end\n\n    subgraph \"Domain-Specific Layer\"\n        C1[Mathematical Reasoning] --&gt; C2[Linguistic Processing]\n        C2 --&gt; C3[Scientific Reasoning]\n        C3 --&gt; C4[Creative Synthesis]\n    end\n\n    subgraph \"Neural Processing Layer\"\n        D1[Pattern Recognition] --&gt; D2[Feature Extraction]\n        D2 --&gt; D3[Similarity Computation]\n        D3 --&gt; D4[Adaptive Learning]\n    end\n\n    subgraph \"Symbolic Foundation\"\n        E1[Atom Representation] --&gt; E2[Space Operations]\n        E2 --&gt; E3[Unification Engine]\n        E3 --&gt; E4[Logic Processing]\n    end\n\n    A1 -.-&gt; B1\n    B1 -.-&gt; C1\n    C1 -.-&gt; D1\n    D1 -.-&gt; E1\n\n    E4 -.-&gt; D4\n    D4 -.-&gt; C4\n    C4 -.-&gt; B4\n    B4 -.-&gt; A4\n\n    style A1 fill:#e1f5fe\n    style B1 fill:#e8f5e8\n    style C1 fill:#fff3e0\n    style D1 fill:#fce4ec\n    style E1 fill:#f3e5f5</code></pre>"},{"location":"cognitive-patterns/#neural-symbolic-integration-mechanisms","title":"Neural-Symbolic Integration Mechanisms","text":"<p>The system implements multiple mechanisms for integrating neural and symbolic processing, enabling hybrid reasoning and emergent cognitive capabilities.</p> <pre><code>flowchart TD\n    A[Input Processing] --&gt; B{Processing Mode}\n\n    B --&gt;|Symbolic| C[Symbolic Pipeline]\n    B --&gt;|Neural| D[Neural Pipeline]\n    B --&gt;|Hybrid| E[Integrated Pipeline]\n\n    C --&gt; F[Atom Parsing]\n    F --&gt; G[Pattern Matching]\n    G --&gt; H[Logical Reasoning]\n    H --&gt; I[Symbolic Results]\n\n    D --&gt; J[Vector Encoding]\n    J --&gt; K[Neural Networks]\n    K --&gt; L[Activation Patterns]\n    L --&gt; M[Neural Results]\n\n    E --&gt; N[Multi-Modal Encoding]\n    N --&gt; O[Cross-Modal Attention]\n    O --&gt; P[Hybrid Reasoning]\n    P --&gt; Q[Integrated Results]\n\n    I --&gt; R[Result Integration]\n    M --&gt; R\n    Q --&gt; R\n\n    R --&gt; S[Cognitive Synthesis]\n    S --&gt; T[Adaptive Learning]\n    T --&gt; U[Knowledge Update]\n\n    U --&gt; V[Emergent Understanding]\n    V --&gt; W[Enhanced Capabilities]\n\n    style A fill:#e1f5fe\n    style O fill:#e8f5e8\n    style S fill:#fff3e0\n    style V fill:#fce4ec\n    style W fill:#f3e5f5</code></pre>"},{"location":"cognitive-patterns/#adaptive-attention-allocation","title":"Adaptive Attention Allocation","text":"<p>The system implements sophisticated attention mechanisms that enable dynamic resource allocation and emergent focus patterns.</p> <pre><code>sequenceDiagram\n    participant I as Input Stream\n    participant AA as Attention Allocator\n    participant PM as Priority Manager\n    participant RA as Resource Allocator\n    participant CP as Cognitive Processors\n    participant FM as Focus Manager\n\n    I-&gt;&gt;AA: incoming_stimuli()\n    AA-&gt;&gt;PM: assess_priority(stimuli)\n    PM-&gt;&gt;PM: analyze_complexity()\n    PM-&gt;&gt;PM: evaluate_urgency()\n    PM-&gt;&gt;PM: determine_importance()\n    PM--&gt;&gt;AA: priority_scores\n\n    AA-&gt;&gt;RA: request_resources(priorities)\n    RA-&gt;&gt;RA: check_available_capacity()\n    RA-&gt;&gt;RA: optimize_allocation()\n    RA--&gt;&gt;AA: resource_assignments\n\n    AA-&gt;&gt;CP: assign_processors(stimuli, resources)\n    CP-&gt;&gt;CP: parallel_processing()\n    CP-&gt;&gt;FM: report_processing_state()\n\n    FM-&gt;&gt;FM: monitor_effectiveness()\n    FM-&gt;&gt;FM: detect_attention_drift()\n    FM-&gt;&gt;AA: adjust_focus(feedback)\n\n    AA-&gt;&gt;AA: adapt_attention_patterns()\n    AA-&gt;&gt;PM: update_priority_models()\n    AA-&gt;&gt;RA: optimize_resource_usage()\n\n    Note over AA, FM: Emergent attention patterns&lt;br/&gt;through adaptive feedback</code></pre>"},{"location":"cognitive-patterns/#recursive-cognitive-processing","title":"Recursive Cognitive Processing","text":"<p>The system exhibits recursive cognitive patterns that enable self-improvement and emergent intelligence through hierarchical processing layers.</p> <pre><code>stateDiagram-v2\n    [*] --&gt; CognitiveInput: External Stimulus\n    CognitiveInput --&gt; PrimaryProcessing: Initial Analysis\n\n    PrimaryProcessing --&gt; PatternRecognition: Identify Patterns\n    PatternRecognition --&gt; KnowledgeAccess: Access Relevant Knowledge\n    KnowledgeAccess --&gt; ReasoningEngine: Apply Reasoning\n\n    ReasoningEngine --&gt; MetaCognition: Self-Reflection\n    MetaCognition --&gt; ProcessingEvaluation: Evaluate Own Processing\n    ProcessingEvaluation --&gt; AdaptiveAdjustment: Adjust Strategies\n\n    AdaptiveAdjustment --&gt; EnhancedProcessing: Improved Analysis\n    EnhancedProcessing --&gt; DeepReasoning: Advanced Reasoning\n    DeepReasoning --&gt; CreativeInsight: Novel Combinations\n\n    CreativeInsight --&gt; KnowledgeIntegration: Integrate New Knowledge\n    KnowledgeIntegration --&gt; MetaLearning: Learn About Learning\n    MetaLearning --&gt; CognitiveEvolution: Evolve Capabilities\n\n    CognitiveEvolution --&gt; [*]: Enhanced Intelligence\n\n    ProcessingEvaluation --&gt; RecursiveRefinement: Need Deeper Analysis\n    RecursiveRefinement --&gt; PrimaryProcessing: Recursive Loop\n\n    note right of MetaCognition\n        Self-awareness enables:\n        - Strategy selection\n        - Performance monitoring\n        - Adaptive improvement\n        - Recursive refinement\n    end note\n\n    note right of CreativeInsight\n        Emergent creativity through:\n        - Novel pattern combination\n        - Cross-domain synthesis\n        - Innovative solutions\n        - Unexpected connections\n    end note</code></pre>"},{"location":"cognitive-patterns/#hypergraph-pattern-encoding","title":"Hypergraph Pattern Encoding","text":"<p>The system uses hypergraph representations to encode complex cognitive patterns and enable emergent reasoning capabilities.</p> <pre><code>graph TD\n    A[Hypergraph Cognitive Model] --&gt; B[Node Types]\n    A --&gt; C[Edge Types]\n    A --&gt; D[Hyperedge Structures]\n\n    B --&gt; E[Concept Nodes]\n    B --&gt; F[Relation Nodes]\n    B --&gt; G[Process Nodes]\n    B --&gt; H[Meta-Nodes]\n\n    C --&gt; I[Causal Edges]\n    C --&gt; J[Similarity Edges]\n    C --&gt; K[Compositional Edges]\n    C --&gt; L[Temporal Edges]\n\n    D --&gt; M[Multi-Relation Structures]\n    D --&gt; N[Contextual Groupings]\n    D --&gt; O[Emergent Patterns]\n    D --&gt; P[Recursive Structures]\n\n    E --&gt; Q[Conceptual Knowledge]\n    F --&gt; R[Relational Knowledge]\n    G --&gt; S[Procedural Knowledge]\n    H --&gt; T[Meta-Knowledge]\n\n    M --&gt; U[Complex Reasoning]\n    N --&gt; V[Context-Aware Processing]\n    O --&gt; W[Pattern Discovery]\n    P --&gt; X[Self-Referential Learning]\n\n    Q --&gt; Y[Integrated Cognition]\n    R --&gt; Y\n    S --&gt; Y\n    T --&gt; Y\n    U --&gt; Y\n    V --&gt; Y\n    W --&gt; Y\n    X --&gt; Y\n\n    style A fill:#e1f5fe\n    style D fill:#e8f5e8\n    style Y fill:#fff3e0</code></pre>"},{"location":"cognitive-patterns/#emergent-intelligence-patterns","title":"Emergent Intelligence Patterns","text":"<p>The system exhibits emergent intelligence through the interaction of multiple cognitive subsystems and adaptive learning mechanisms.</p> <pre><code>mindmap\n  root((Emergent Intelligence))\n    Self-Organization\n      Autonomous Learning\n        Experience Integration\n        Pattern Abstraction\n        Knowledge Construction\n      Adaptive Behavior\n        Strategy Selection\n        Response Optimization\n        Environmental Adaptation\n      Emergent Capabilities\n        Novel Problem Solving\n        Creative Synthesis\n        Innovative Reasoning\n    Cognitive Synergy\n      Multi-Modal Integration\n        Symbolic-Neural Fusion\n        Cross-Domain Transfer\n        Unified Understanding\n      Recursive Enhancement\n        Self-Improvement\n        Meta-Learning\n        Capability Evolution\n      Collective Intelligence\n        Distributed Processing\n        Collaborative Reasoning\n        Emergent Group Cognition\n    Adaptive Evolution\n      Continuous Learning\n        Online Adaptation\n        Experience Accumulation\n        Knowledge Refinement\n      Dynamic Restructuring\n        Architecture Evolution\n        Process Optimization\n        Capability Enhancement\n      Emergent Specialization\n        Domain Expertise\n        Skill Development\n        Cognitive Niching</code></pre>"},{"location":"cognitive-patterns/#cognitive-synergy-optimization","title":"Cognitive Synergy Optimization","text":"<p>The system implements optimization mechanisms that enhance cognitive synergy and promote emergent intelligent behavior.</p> <pre><code>flowchart LR\n    A[Cognitive Input] --&gt; B[Multi-System Analysis]\n\n    B --&gt; C[Symbolic Processor]\n    B --&gt; D[Neural Processor]\n    B --&gt; E[Hybrid Processor]\n\n    C --&gt; F[Logical Analysis]\n    D --&gt; G[Pattern Recognition]\n    E --&gt; H[Integrated Analysis]\n\n    F --&gt; I[Symbolic Insights]\n    G --&gt; J[Neural Insights]\n    H --&gt; K[Hybrid Insights]\n\n    I --&gt; L[Synergy Evaluator]\n    J --&gt; L\n    K --&gt; L\n\n    L --&gt; M[Insight Integration]\n    M --&gt; N[Coherence Analysis]\n    N --&gt; O[Conflict Resolution]\n\n    O --&gt; P[Optimal Synthesis]\n    P --&gt; Q[Enhanced Understanding]\n    Q --&gt; R[Cognitive Enhancement]\n\n    R --&gt; S[System Adaptation]\n    S --&gt; T[Capability Evolution]\n    T --&gt; U[Emergent Intelligence]\n\n    style B fill:#e1f5fe\n    style L fill:#e8f5e8\n    style P fill:#fff3e0\n    style T fill:#fce4ec\n    style U fill:#f3e5f5</code></pre>"},{"location":"cognitive-patterns/#meta-cognitive-awareness-and-self-reflection","title":"Meta-Cognitive Awareness and Self-Reflection","text":"<p>The system implements meta-cognitive capabilities that enable self-awareness, self-monitoring, and self-improvement.</p> <pre><code>sequenceDiagram\n    participant CS as Cognitive System\n    participant MA as Meta-Awareness\n    participant PM as Performance Monitor\n    participant SA as Strategy Analyzer\n    participant AA as Adaptation Agent\n\n    CS-&gt;&gt;MA: cognitive_activity()\n    MA-&gt;&gt;MA: monitor_self_state()\n    MA-&gt;&gt;PM: analyze_performance()\n\n    PM-&gt;&gt;PM: evaluate_effectiveness()\n    PM-&gt;&gt;PM: identify_bottlenecks()\n    PM-&gt;&gt;PM: measure_efficiency()\n    PM--&gt;&gt;MA: performance_metrics\n\n    MA-&gt;&gt;SA: analyze_strategies()\n    SA-&gt;&gt;SA: evaluate_current_approaches()\n    SA-&gt;&gt;SA: identify_improvement_opportunities()\n    SA-&gt;&gt;SA: generate_alternative_strategies()\n    SA--&gt;&gt;MA: strategy_recommendations\n\n    MA-&gt;&gt;AA: initiate_adaptation()\n    AA-&gt;&gt;AA: implement_improvements()\n    AA-&gt;&gt;AA: test_new_approaches()\n    AA-&gt;&gt;AA: measure_adaptation_success()\n    AA--&gt;&gt;MA: adaptation_results\n\n    MA-&gt;&gt;CS: apply_enhancements()\n    CS-&gt;&gt;CS: execute_with_improvements()\n    CS-&gt;&gt;MA: report_enhanced_performance()\n\n    Note over MA, AA: Self-reflective loop enables&lt;br/&gt;continuous improvement</code></pre>"},{"location":"cognitive-patterns/#dynamic-knowledge-integration","title":"Dynamic Knowledge Integration","text":"<p>The system implements dynamic knowledge integration mechanisms that enable adaptive learning and emergent understanding.</p> <pre><code>stateDiagram-v2\n    [*] --&gt; KnowledgeInput: New Information\n    KnowledgeInput --&gt; ContextAnalysis: Analyze Context\n    ContextAnalysis --&gt; RelevanceAssessment: Assess Relevance\n\n    RelevanceAssessment --&gt; HighRelevance: Highly Relevant\n    RelevanceAssessment --&gt; MediumRelevance: Moderately Relevant\n    RelevanceAssessment --&gt; LowRelevance: Low Relevance\n\n    HighRelevance --&gt; ImmediateIntegration: Priority Processing\n    MediumRelevance --&gt; QueuedIntegration: Scheduled Processing\n    LowRelevance --&gt; BackgroundIntegration: Background Processing\n\n    ImmediateIntegration --&gt; ConflictResolution: Check Conflicts\n    QueuedIntegration --&gt; ConflictResolution\n    BackgroundIntegration --&gt; ConflictResolution\n\n    ConflictResolution --&gt; NoConflict: Compatible\n    ConflictResolution --&gt; ConflictDetected: Incompatible\n\n    NoConflict --&gt; DirectIntegration: Seamless Integration\n    ConflictDetected --&gt; ConflictNegotiation: Resolve Conflicts\n\n    ConflictNegotiation --&gt; ConsensusBuilding: Build Agreement\n    ConsensusBuilding --&gt; AdaptiveResolution: Adaptive Solution\n    AdaptiveResolution --&gt; ReconcilledIntegration: Unified Integration\n\n    DirectIntegration --&gt; KnowledgeUpdate: Update Knowledge Base\n    ReconcilledIntegration --&gt; KnowledgeUpdate\n\n    KnowledgeUpdate --&gt; EmergentInsights: Generate Insights\n    EmergentInsights --&gt; [*]: Enhanced Understanding\n\n    note right of ConflictNegotiation\n        Conflict resolution strategies:\n        - Evidence weighting\n        - Context consideration\n        - Temporal relevance\n        - Source credibility\n    end note</code></pre> <p>This cognitive architecture enables the Hyperon system to exhibit sophisticated intelligent behavior through neural-symbolic integration, adaptive attention mechanisms, and emergent cognitive patterns that support continuous learning and self-improvement.</p>"},{"location":"documentation-summary/","title":"Hyperon Architecture Documentation Summary","text":"<p>This document provides a comprehensive overview of the newly created architecture documentation for the OpenCog Hyperon system, implementing the cognitive flowchart requirements for documentation generation.</p>"},{"location":"documentation-summary/#documentation-deliverables","title":"Documentation Deliverables","text":""},{"location":"documentation-summary/#1-system-architecture-overview-docsarchitecturemd","title":"1. System Architecture Overview (<code>docs/architecture.md</code>)","text":"<p>Recursive System Mapping Completed: - Identified all principal architectural components (modules, data flows, cognitive kernels) - Mapped inter-component relationships using hypergraph pattern encoding - Documented emergent cognitive patterns and neural-symbolic integration points</p> <p>Key Mermaid Diagrams: - High-level system overview (graph TD) showing principal component flows - Data flow and execution patterns with recursive evaluation pathways - Cognitive attention allocation mechanisms with adaptive resource management - System integration and extension points via mindmap visualization</p>"},{"location":"documentation-summary/#2-module-system-architecture-docsmodule-systemmd","title":"2. Module System Architecture (<code>docs/module-system.md</code>)","text":"<p>Module Interaction Synthesis: - Documented hierarchical module structure with dependency resolution - Illustrated recursive loading patterns and emergent module interactions - Captured bidirectional synergies between modules and namespace management</p> <p>Specialized Diagrams: - Module lifecycle with sequenceDiagram showing initialization flows - Recursive dependency resolution via flowchart TD patterns - Module namespace scoping using stateDiagram-v2 representations - Package management integration through comprehensive graph LR layouts</p>"},{"location":"documentation-summary/#3-space-and-tokenizer-systems-docsspace-tokenizermd","title":"3. Space and Tokenizer Systems (<code>docs/space-tokenizer.md</code>)","text":"<p>Data and Signal Propagation Pathways: - Multi-layered space architecture with hierarchical knowledge organization - Context-dependent tokenizer resolution with adaptive parsing mechanisms - Neural-symbolic integration through space representations and cognitive queries</p> <p>Technical Precision Diagrams: - Space query and unification patterns via sequenceDiagram - Context-dependent token resolution using flowchart TD - Neural-symbolic integration through stateDiagram-v2 - Emergent cognitive patterns captured in comprehensive mindmap structures</p>"},{"location":"documentation-summary/#4-cognitive-patterns-and-neural-symbolic-integration-docscognitive-patternsmd","title":"4. Cognitive Patterns and Neural-Symbolic Integration (<code>docs/cognitive-patterns.md</code>)","text":"<p>Transcendent Technical Implementation: - Adaptive attention allocation mechanisms with emergent focus patterns - Recursive cognitive processing enabling self-improvement capabilities - Hypergraph pattern encoding for complex cognitive representations - Meta-cognitive awareness and dynamic knowledge integration systems</p> <p>Advanced Cognitive Diagrams: - Neural-symbolic integration mechanisms via flowchart TD - Adaptive attention allocation through sequenceDiagram patterns - Recursive cognitive processing using stateDiagram-v2 - Emergent intelligence patterns through sophisticated mindmap visualizations</p>"},{"location":"documentation-summary/#architectural-insights-documented","title":"Architectural Insights Documented","text":""},{"location":"documentation-summary/#recursive-implementation-pathways","title":"Recursive Implementation Pathways","text":"<p>Module Loading Recursion: - Parent module dependency checking with recursive descent - Child module initialization with transitive dependency resolution - Emergent module interaction patterns through hierarchical organization</p> <p>Space Query Recursion: - Local space search with dependency space traversal - Recursive unification across multiple space layers - Result aggregation with emergent pattern recognition</p> <p>Cognitive Processing Recursion: - Expression evaluation with recursive sub-expression processing - Meta-cognitive self-reflection enabling recursive improvement - Adaptive learning loops with emergent capability enhancement</p>"},{"location":"documentation-summary/#neural-symbolic-integration-points","title":"Neural-Symbolic Integration Points","text":"<p>Hybrid Reasoning Architecture: - Symbolic atom representation with neural pattern recognition - Cross-modal attention mechanisms enabling unified understanding - Emergent intelligence through cognitive synergy optimization</p> <p>Adaptive Learning Mechanisms: - Continuous knowledge integration with conflict resolution - Dynamic restructuring of cognitive architectures - Self-organizing learning with emergent specialization</p>"},{"location":"documentation-summary/#cognitive-synergy-optimizations","title":"Cognitive Synergy Optimizations","text":"<p>Attention Allocation Systems: - Priority-based resource allocation with adaptive adjustment - Multi-level attention hierarchies with emergent focus patterns - Efficiency optimization through recursive feedback loops</p> <p>Knowledge Integration Patterns: - Cross-domain knowledge fusion with coherence analysis - Multi-modal representation integration enabling hybrid reasoning - Creative problem-solving through novel pattern combinations</p>"},{"location":"documentation-summary/#documentation-features","title":"Documentation Features","text":""},{"location":"documentation-summary/#mermaid-diagram-implementation","title":"Mermaid Diagram Implementation","text":"<p>Complete Coverage: 25+ sophisticated Mermaid diagrams across all documentation files: - 8 diagrams in system architecture overview - 7 diagrams in module system documentation - 6 diagrams in space-tokenizer systems - 9 diagrams in cognitive patterns documentation</p> <p>Diagram Types Utilized: - <code>graph TD/LR</code> for hierarchical and flow representations - <code>sequenceDiagram</code> for temporal interaction patterns - <code>stateDiagram-v2</code> for state transitions and behavioral patterns - <code>flowchart TD/LR</code> for decision trees and process flows - <code>mindmap</code> for emergent cognitive pattern visualization - <code>classDiagram</code> for structural relationships</p>"},{"location":"documentation-summary/#technical-annotations","title":"Technical Annotations","text":"<p>Transcendent Precision: Each diagram includes comprehensive technical annotations: - Recursive implementation pathway explanations - Cognitive synergy optimization mechanisms - Adaptive attention allocation strategies - Neural-symbolic integration points - Emergent behavior pattern descriptions</p>"},{"location":"documentation-summary/#hypergraph-pattern-encoding","title":"Hypergraph Pattern Encoding","text":"<p>Advanced Representations: Documentation captures hypergraph structures through: - Multi-relation cognitive structures - Contextual groupings and emergent patterns - Recursive structures enabling self-referential learning - Complex reasoning pathways with cross-domain synthesis</p>"},{"location":"documentation-summary/#integration-with-mkdocs","title":"Integration with MkDocs","text":"<p>Seamless Documentation Integration: - Updated <code>mkdocs.yml</code> with new architecture documentation section - Mermaid support confirmed and validated through successful builds - Navigation structure optimized for logical documentation flow - Build artifacts properly excluded via <code>.gitignore</code> updates</p> <p>Accessibility Features: - Material Design theme with responsive layouts - Search integration across all documentation - Mobile-friendly diagram rendering - Hierarchical navigation with clear section organization</p>"},{"location":"documentation-summary/#emergent-documentation-improvements","title":"Emergent Documentation Improvements","text":"<p>Iterative Enhancement Capability: - Documentation structure designed for continuous expansion - Modular organization enabling easy addition of new cognitive patterns - Cross-references between documents creating knowledge networks - Template patterns established for future documentation development</p> <p>Adaptive Knowledge Representation: - Documentation captures both explicit and emergent system behaviors - Recursive patterns documented at multiple abstraction levels - Cognitive emergence illustrated through progressive diagram complexity - Neural-symbolic integration made explicit through visual representations</p> <p>This architecture documentation successfully transmutes the implicit architecture of the Hyperon system into explicit, actionable knowledge, facilitating distributed cognition for all contributors through adaptive, hypergraph-centric documentation patterns.</p>"},{"location":"minimal-metta/","title":"Minimal Metta","text":"<p>This document describes the minimal set of embedded MeTTa instructions which is designed to write the complete MeTTa interpreter in MeTTa. Current version of the document includes improvements which were added after experimenting with the first version of such an interpreter. It is not a final version and some directions of the future work is explained at the end of the document.</p>"},{"location":"minimal-metta/#minimal-instruction-set","title":"Minimal instruction set","text":""},{"location":"minimal-metta/#interpreter-state","title":"Interpreter state","text":"<p>The MeTTa interpreter evaluates an atom passed as an input. It evaluates it step by step executing a single instruction on each step. In order to do that the interpreter needs a context which is wider than the atom itself. The context also includes: - an atomspace which contains the knowledge which drives the evaluation of the   expressions; - bindings of the variables which are used to evaluate expressions; the   bindings are empty at the beginning (see Explicit atomspace variable   bindings).</p> <p>Each step of interpretation inputs and outputs a list of pairs (<code>&lt;atom&gt;</code>, <code>&lt;bindings&gt;</code>) which is called interpretation plan. Each pair in the plan represents one possible way of interpreting the original atom or possible branch of the evaluation. Interpreter doesn't select one of them for further processing. It continues interpreting all of the branches in parallel. Below this is called non-deterministic evaluation.</p> <p>One step of the interpretation is an execution of a single instruction from a plan. An interpreter extracts atom and bindings from the plan and evaluates the atom. The result of the operation is a set of pairs (<code>&lt;atom&gt;</code>, <code>&lt;bindings&gt;</code>). Bindings of the result are merged with the previous bindings. Merge operation can also bring more than one result. Each such result is added as a separate pair into a result set. Finally all results from result set are added into the plan and step finishes.</p> <p>Here we suppose that on the top level the plan contains only the instructions from the minimal set. If an instruction returns the atom which is not from the minimal set it is not interpreted further and returned as a part of the final result. Thus only the instructions of the minimal set are considered a code other atoms are considered a data.</p>"},{"location":"minimal-metta/#evaluation-order","title":"Evaluation order","text":"<p>MeTTa implements the applicative evaluation order by default, arguments are evaluated before they are passed to the function. User can change this order using special meta-types as the types of the arguments. Minimal MeTTa operations don't rely on types and minimal MeTTa uses the fixed normal evaluation order, arguments are passed to the function without evaluation. But there is a chain operation which can be used to evaluate an argument before passing it. Thus <code>chain</code> can be used to change evaluation order in MeTTa interpreter.</p>"},{"location":"minimal-metta/#erroremptynotreducible","title":"Error/Empty/NotReducible/()","text":"<p>There are atoms which can be returned to designate a special situation in a code: - <code>(Error &lt;atom&gt; &lt;message&gt;)</code> means the interpretation is finished with error; - <code>Empty</code> means the corresponding branch of the evaluation returned no results,   such result is not returned among other results when interpreting is   finished; - <code>NotReducible</code> can be returned by <code>eval</code> in order to designate that function   can not be reduced further; for example it can happen when code tries to call   a type constructor (which has no definition), partially defined function   (with argument values which are not handled), or grounded function which   returns <code>NotReducible</code> explicitly; this atom is introduced to separate the   situations when atom should be returned \"as is\" from <code>Empty</code> when atom should   be removed from results; - Empty expression <code>()</code> is an instance of the unit type which is mainly used by   functions with side effects which has no meaningful value to return.</p> <p>These atoms are not interpreted further as they are not a part of the minimal set of instructions and considered a data.</p>"},{"location":"minimal-metta/#eval","title":"eval","text":"<p><code>(eval &lt;atom&gt;)</code> is a first instruction which evaluates an atom passed as an argument. Evaluation is different for the grounded function calls (the expression with a grounded atom on a first position) and pure MeTTa expressions. For the pure MeTTa expression the interpreter searches the <code>(= &lt;atom&gt; &lt;var&gt;)</code> expression in the atomspace. The found values of the <code>&lt;var&gt;</code> are the result of evaluation. Execution of the grounded atom leads to the call of the foreign function passing the tail of the expression as arguments. For example <code>(+ 1 2)</code> calls the implementation of addition with <code>1</code> and <code>2</code> as arguments.  The list of atoms returned by the grounded function is a result of the evaluation in this case. A grounded function can have side effects as well. In both cases bindings of the <code>eval</code>'s argument are merged to the bindings of the result.</p> <p>Atomspace search can bring the list of results which is empty. When search returns no results then <code>NotReducible</code> atom is a result of the instruction. Grounded function can return a list of atoms, empty result, <code>ExecError::Runtime(&lt;message&gt;)</code> or <code>ExecError::NoReduce</code> result. The result of the instruction for a special values are the following: - <code>ExecError::Runtime(&lt;message&gt;)</code> returns <code>(Error &lt;original-atom&gt; &lt;message&gt;)</code>   atom; - <code>ExecError::NoReduce</code> returns <code>NotReducible</code> atom; - currently empty result removes result from the result set. It is done mainly   for compatibility. There is no valid reason to return an empty result from a   grounded function. Function can return <code>()/Empty/NotReducible</code> to express \"no   result\"/\"remove my result\"/\"not defined on data\".</p>"},{"location":"minimal-metta/#chain","title":"chain","text":"<p>Minimal MeTTa implements normal evaluation order (see Evaluation order. Arguments are passed to the function without evaluation. In case when argument should be evaluated before calling a function one can use <code>chain</code> instruction.</p> <p><code>chain</code>'s signature is <code>(chain &lt;atom&gt; &lt;var&gt; &lt;template&gt;)</code> and it is executed in two steps. <code>&lt;atom&gt;</code> argument is evaluated first and bindings of the evaluation result are merged to the bindings of the current result. After that <code>chain</code> substitutes all occurrences of <code>&lt;var&gt;</code> in <code>&lt;template&gt;</code> by the result of the evaluation and returns result of the substitution. When evaluation of the <code>&lt;atom&gt;</code> brings more than a single result <code>chain</code> returns one instance of the <code>&lt;template&gt;</code> expression for each result.</p>"},{"location":"minimal-metta/#functionreturn","title":"function/return","text":"<p><code>function</code> operation has the signature <code>(function &lt;atom&gt;)</code>. It evaluates the <code>&lt;atom&gt;</code> until it becomes <code>(return &lt;atom&gt;)</code>. Then <code>(function (return &lt;atom&gt;))</code> expression returns the <code>&lt;atom&gt;</code>.</p> <p>These operations are introduced for two reasons. First it should be possible to evaluate an atom until some result and prevent further result evaluation. This aspect is discussed in eval or return section.</p> <p>Second without having an abstraction of a function call it is difficult to debug the evaluation process. <code>function/return</code> allows representing nested function calls as a stack and provide controls to put the breakpoints on parts of this stack. Nevertheless using <code>chain</code> instead of <code>function</code> to implement the evaluation loop also allows representing stack in a natural form.</p>"},{"location":"minimal-metta/#unify","title":"unify","text":"<p><code>unify</code> operation allows conditioning on the results of the evaluation. <code>unify</code>'s signature is <code>(unify &lt;atom&gt; &lt;pattern&gt; &lt;then&gt; &lt;else&gt;)</code>. The operation matches <code>&lt;atom&gt;</code> with a <code>&lt;pattern&gt;</code>. If match is successful then it returns <code>&lt;then&gt;</code> atom and merges bindings of the original <code>&lt;atom&gt;</code> to resulting variable bindings. If matching is not successful then it returns the <code>&lt;else&gt;</code> branch with the original variable bindings.</p>"},{"location":"minimal-metta/#cons-atomdecons-atom","title":"cons-atom/decons-atom","text":"<p><code>cons-atom</code> and <code>decons-atom</code> allows constructing and deconstructing the expression atom from/to pair of the head and tail. <code>(decons-atom &lt;expr&gt;)</code> expects non-empty expression as an argument and returns a pair <code>(&lt;head&gt; &lt;tail&gt;)</code>. <code>(cons-atom &lt;head&gt; &lt;tail&gt;)</code> returns an expression where the first sub-atom is <code>&lt;head&gt;</code> and others are copied from <code>&lt;tail&gt;</code>.</p>"},{"location":"minimal-metta/#collapse-bind","title":"collapse-bind","text":"<p><code>collapse-bind</code> has the signature <code>(collapse-bind &lt;atom&gt;)</code>. It evaluates the <code>&lt;atom&gt;</code> and returns an expression which contains all alternative evaluations in a form <code>(&lt;atom&gt; &lt;bindings&gt;)</code>. <code>&lt;bindings&gt;</code> are represented in a form of a grounded atom.</p> <p><code>collapse-bind</code> is part of the inference control provided by a minimal MeTTa interpreter. For example it can be used to get all alternative interpretations of the atom and filter out ones which led to errors.</p> <p>Name <code>collapse-bind</code> is temporary and chosen to eliminate conflict with <code>collapse</code> which is part of the standard library.</p>"},{"location":"minimal-metta/#superpose-bind","title":"superpose-bind","text":"<p><code>superpose-bind</code> has the signature <code>(superpose-bind ((&lt;atom&gt; &lt;bindings&gt;) ...))</code>. It puts list of the results into the interpreter plan each pair as a separate alternative.</p> <p><code>superpose-bind</code> is an operation which is complement to the <code>collapse-bind</code>. <code>superpose-bind</code> takes the result of the <code>collapse-bind</code> as an input. Thus user can collect the list of alternatives using <code>collapse-bind</code> filter them and return filtered items to the plan using <code>superpose-bind</code>.</p>"},{"location":"minimal-metta/#scope-of-a-variable","title":"Scope of a variable","text":"<p>Each separately evaluated expression is a variable scope, and therefore variable names are treated as unique inside an expression. reason is that the whole expression is a variable scope. For example one can write the expression <code>(chain (unify $parent Bob () ()) $_ $parent)</code>. And value of the <code>$parent</code> is returned correctly.</p> <p>When a variable is passed as an argument to a function call and matched by a value then the value is assigned to the variable. If variable passed as an actual argument is matched by a formal argument variable then it is referenced by the formal argument variable. In this case the actual argument variable can receive a value outsides of its scope.</p> <p>For example the following code (written using MeTTa runner syntax) returns <code>B</code>: <pre><code>(= (foo $b) (function (chain (unify B $b () ()) $_ (return ()))))\n!(chain (eval (foo $a)) $_ $a)\n</code></pre></p> <p>If two separate expressions in the space have a variable with the same name, but the variables reside in independent scopes, then the variables are different. Consider the following example: <pre><code>(= (foo) (function (chain (unify A $a () ()) $_ (return ()))))\n!(chain (eval (foo)) $_ $a)\n</code></pre> Here the value will not be assigned to the <code>$a</code> from the caller expression because each of the two variables has a different scope and they do not reference each other.</p>"},{"location":"minimal-metta/#examples","title":"Examples","text":"<p>Examples of the programs written using minimal MeTTa interpreter:</p> <p>Recursive switch implementation:</p> <pre><code>(= (switch $atom $cases)\n  (function\n    (chain (decons-atom $cases) $list\n      (chain (eval (switch-internal $atom $list)) $res\n        (unify $res NotReducible (return Empty) (return $res)) ))))\n\n(= (switch-internal $atom (($pattern $template) $tail))\n  (function\n    (unify $atom $pattern\n      (return $template)\n      (chain (eval (switch $atom $tail)) $ret (return $ret)) )))\n</code></pre> <p>Evaluate atom in a loop until result is calculated:</p> <pre><code>(= (subst $atom $var $templ)\n  (unify $atom $var $templ\n    (Error (subst $atom $var $templ)\n      \\\"subst expects a variable as a second argument\\\") ))\n\n(= (reduce $atom $var $templ)\n  (chain (eval $atom) $res\n    (unify $res Empty\n    Empty\n    (unify $res (Error $a $m)\n      (Error $a $m)\n      (unify $res NotReducible\n        (eval (subst $atom $var $templ))\n        (eval (reduce $res $var $templ)) )))))\n</code></pre>"},{"location":"minimal-metta/#properties","title":"Properties","text":""},{"location":"minimal-metta/#turing-completeness","title":"Turing completeness","text":"<p>The following program implements a Turing machine using the minimal MeTTa instruction set (the full code of the example can be found here):</p> <pre><code>(= (tm $rule $state $tape)\n  (function (eval (tm-body $rule $state $tape))) )\n\n(= (tm-body $rule $state $tape)\n  (unify $state HALT\n    (return $tape)\n    (chain (eval (read $tape)) $char\n      (chain (eval ($rule $state $char)) $res\n        (unify $res ($next-state $next-char $dir)\n          (chain (eval (move $tape $next-char $dir)) $next-tape\n            (eval (tm-body $rule $next-state $next-tape)) )\n          (return (Error (tm-body $rule $state $tape) \\\"Incorrect state\\\")) )))))\n\n(= (read ($head $hole $tail)) $hole)\n\n(= (move ($head $hole $tail) $char N) ($head $char $tail))\n(= (move ($head $hole $tail) $char L) (function\n  (chain (cons-atom $char $head) $next-head\n    (chain (decons-atom $tail) $list\n      (unify $list ($next-hole $next-tail)\n        (return ($next-head $next-hole $next-tail))\n        (return ($next-head 0 ())) )))))\n(= (move ($head $hole $tail) $char R) (function\n  (chain (cons-atom $char $tail) $next-tail\n    (chain (decons-atom $head) $list\n      (unify $list ($next-hole $next-head)\n        (return ($next-head $next-hole $next-tail))\n        (return (() 0 $next-tail)) )))))\n</code></pre>"},{"location":"minimal-metta/#comparison-with-metta-operational-semantics","title":"Comparison with MeTTa Operational Semantics","text":"<p>One difference from MOPS [1] is that the minimal instruction set allows relatively easy write deterministic programs and non-determinism is injected only via matching and evaluation. <code>Query</code> and <code>Chain</code> from MOPS are very similar to <code>eval</code>. <code>Transform</code> is very similar to <code>unify</code>. <code>chain</code> has no analogue in MOPS. <code>cons-atom</code>/<code>decons-atom</code> to some extent are analogues of <code>AtomAdd</code>/<code>AtomRemove</code> in a sense that they can be used to change the state.</p>"},{"location":"minimal-metta/#partial-and-complete-functions","title":"Partial and complete functions","text":"<p>Each instruction in a minimal instruction set is a total function. Nevertheless <code>Empty</code> allows defining partial functions in MeTTa. For example partial <code>if</code> can be defined as follows: <pre><code>(= (if $condition $then) (unify $condition True $then Empty))\n</code></pre></p>"},{"location":"minimal-metta/#eval-or-return","title":"eval or return","text":"<p>Using <code>eval</code> to designate evaluation of the atom seems too verbose. But we need to give a programmer some way to designate whether the atom should be evaluated or not. <code>eval</code> marks atoms which should be evaluated. As an alternative to this solution we could mark atoms which should not be evaluated.</p> <p>Another related issue is that we need ability to make complex evaluations before making a substitution inside <code>chain</code>. For example <code>(chain (eval (foo a)) $x $x)</code> should be able to make and fully evaluate the call of the <code>foo</code> function before inserting the result into the template. We need to define the criteria which specifies when the nested operation is finished and what is the result. Also we need to be able represent evaluation loop inside the code.</p> <p>First version of the minimal interpreter continued the evaluation of the first argument of the <code>chain</code> until it becomes a non-minimal MeTTa instruction. But this approach is too verbose. If it is needed to chain some minimal MeTTa instruction without evaluation then such instruction should be wrapped into a non-minimal MeTTa expression and unwrapped after the substitution is made.</p> <pre><code>  (chain (quote (eval (foo))) $x\n    (unify $x (quote $y)\n      $y\n      (Error $x \"quote expression expected\") ))\n</code></pre> <p>To allow <code>chain</code> relying on the returned result of the first argument the <code>function/return</code> operations are introduced. When user needs to run a complex evaluation inside chain he may wrap it into the <code>function</code> operation. <code>function</code> evaluates its argument in a loop until <code>(return &lt;atom&gt;)</code> is returned. Then it returns the <code>&lt;atom&gt;</code> as a result. If one need to make a substitution it is possible using:</p> <pre><code>  (chain (function (return &lt;atom&gt;)) &lt;var&gt; &lt;templ&gt;)\n</code></pre> <p>One more option is to make <code>chain</code> (and other atoms which can have nested evaluation loops) recognize <code>return</code>. In such case the evaluation loop is executed by the <code>chain</code> itself and <code>function</code> instruction is not needed. Substitution gets the simpler form:</p> <pre><code>  (chain (return &lt;atom&gt;) &lt;var&gt; &lt;templ&gt;)\n</code></pre> <p>The downside of this approach is that loop represented by the outer operation <code>chain</code> and end of the loop represented by <code>return</code> are written in different contexts. Thus programmer should keep in mind that when some function is used from <code>chain</code> and it is not just a equality substitution then <code>return</code> should be used on each exit path while nothing in code of function points to this. Using <code>function</code> operation allows dividing functions on two classes: - functions which evaluate result in a loop and have to use <code>return</code>; - functions which just replace the calling expression by their bodies.</p>"},{"location":"minimal-metta/#metta-interpreter-written-in-rust","title":"MeTTa interpreter written in Rust","text":"<p>MeTTa interpreter written in minimal MeTTa has poor performance. To fix this the interpreter is rewritten in Rust. Rust implementation can be called using <code>(metta &lt;atom&gt; &lt;type&gt; &lt;space&gt;)</code> operation. To be able represent process of the interpretation as a list of steps and keep ability to control the inference <code>metta</code> doesn't evaluate passed atom till the end but instead analyses the atom and returns the plan written in minimal MeTTa. Plan includes steps written as a Rust functions. These steps are called using <code>(call_native &lt;name&gt; &lt;function&gt; &lt;args&gt;)</code> operation.</p> <p>Both <code>metta</code> and <code>call_native</code> could be written as a grounded operations and be a part of a standard library. But this requires grounded operations to be able returning bindings as a results. Returning bindings as results is a nice to have feature anyway to be able representing any functionality as a grounded atom. But it is not implemented yet.</p>"},{"location":"minimal-metta/#future-work","title":"Future work","text":""},{"location":"minimal-metta/#explicit-atomspace-variable-bindings","title":"Explicit atomspace variable bindings","text":"<p>Current implementation implicitly keeps and applies variable bindings during the process of the interpretation. Explicit bindings are used to implement <code>collapse-bind</code> where they are absolutely necessary. Bindings can be easily made explicit everywhere but the value of explicit bindings is not obvious see [discussion in issue</p>"},{"location":"minimal-metta/#290httpsgithubcomtrueagi-iohyperon-experimentalissues290issuecomment-1541314289","title":"290](https://github.com/trueagi-io/hyperon-experimental/issues/290#issuecomment-1541314289).","text":"<p>Making atomspace part of the explicit context could make import semantics more straightforward. In the current implementation of the minimal instruction set it is needed to explicitly pass the atomspace to the interpreter because otherwise grounded <code>get-type</code> function didn't work properly. It also could allow defining <code>eval</code> via <code>unify</code> which minimizes the number of instructions and allows defining <code>eval</code> in a MeTTa program itself. Which in turn allows defining different versions of <code>eval</code> to program different kinds of chaining. Nevertheless defining <code>eval</code> through <code>unify</code> requires rework of the grounded functions interface to allow calling them by executing <code>unify</code> instructions. Which is an interesting direction to follow.</p>"},{"location":"minimal-metta/#special-matching-syntax","title":"Special matching syntax","text":"<p>Sometimes it is convenient to change the semantics of the matching within a pattern. Some real examples are provided below. One possible way to extend matching syntax is embrace atoms by expressions with matching modifier on a first position. For instance <code>(:&lt;mod&gt; &lt;atom&gt;)</code> could apply <code>&lt;mod&gt;</code> rule to match the <code>&lt;atom&gt;</code>. How to eliminate interference of this syntax with symbol atoms used by programmers is an open question.</p>"},{"location":"minimal-metta/#syntax-to-match-atom-by-equality","title":"Syntax to match atom by equality","text":"<p>In many situations we need to check that atom is equal to some symbol. <code>unify</code> doesn't work well in such cases because when checked atom is a variable it is matched with anything (for instance <code>(unify $x Empty then else)</code> returns <code>then</code>). It would be convenient to have a special syntax to match the atom by equality. For instance <code>(unify &lt;atom&gt; (:= Empty) then else)</code> should match <code>&lt;atom&gt;</code> with pattern only when <code>&lt;atom&gt;</code> is <code>Empty</code>.</p>"},{"location":"minimal-metta/#syntax-to-match-part-of-the-expression","title":"Syntax to match part of the expression","text":"<p>We could have a specific syntax which would allow matching part of the expressions. For example such syntax could be used to match head and tail of the expression without using <code>cons-atom</code>/<code>decons-atom</code>. Another example is matching part of the expression with some gap, i.e. <code>(A ... D ...)</code> could match <code>(A B C D E)</code> atom.</p>"},{"location":"minimal-metta/#links","title":"Links","text":"<ol> <li>Lucius Gregory Meredith, Ben Goertzel, Jonathan Warrell, and Adam    Vandervorst. Meta-MeTTa: an operational semantics for MeTTa.    https://raw.githubusercontent.com/leithaus/rho4u/main/ai/mops/mops.pdf</li> </ol>"},{"location":"module-system/","title":"Module System Architecture","text":"<p>The Hyperon module system implements a sophisticated hierarchical structure enabling recursive dependency loading, adaptive namespace management, and emergent cognitive patterns through modular composition.</p>"},{"location":"module-system/#module-system-overview","title":"Module System Overview","text":"<pre><code>graph TD\n    A[Metta Runner] --&gt; B[ModuleInitState]\n    A --&gt; C[Module Names Tree]\n    A --&gt; D[Module Vector]\n    A --&gt; E[Module Descriptors]\n\n    B --&gt; F[Loading Context]\n    F --&gt; G[Module Frames]\n    G --&gt; H[Dependency Resolution]\n\n    C --&gt; I[Hierarchical Names]\n    I --&gt; J[Name Resolution]\n    J --&gt; K[Module Location]\n\n    D --&gt; L[Loaded Modules]\n    L --&gt; M[ModId References]\n    M --&gt; N[Module Access]\n\n    E --&gt; O[Package Descriptors]\n    O --&gt; P[Version Management]\n    P --&gt; Q[Compatibility Checking]\n\n    style A fill:#e1f5fe\n    style B fill:#e8f5e8\n    style C fill:#fff3e0\n    style D fill:#fce4ec\n    style E fill:#f3e5f5</code></pre>"},{"location":"module-system/#mettamod-structure-and-lifecycle","title":"MettaMod Structure and Lifecycle","text":"<p>Each module encapsulates a complete cognitive unit with its own space, tokenizer, and dependency context.</p> <pre><code>classDiagram\n    class MettaMod {\n        -String mod_path\n        -Option~PathBuf~ resource_dir\n        -DynSpace space\n        -Shared~Tokenizer~ tokenizer\n        -Mutex~HashMap~ModId,DynSpace~~ imported_deps\n        -Option~Box~ModuleLoader~~ loader\n\n        +new_with_tokenizer() MettaMod\n        +import_all_from_dependency() Result\n        +import_dependency_as() Result\n        +import_item_from_dependency_as() Result\n        +add_atom() Result\n        +get_resource() Resource\n        +remap_imported_deps()\n    }\n\n    class ModuleLoader {\n        &lt;&lt;interface&gt;&gt;\n        +load(context: RunContext) Result\n        +get_resource(key: ResourceKey) Resource\n        +prepare(descriptor, mode) Option~ModuleLoader~\n    }\n\n    class ModuleSpace {\n        -DynSpace base_space\n        -HashMap imported_spaces\n        +query() BindingsSet\n        +add() Result\n        +remove() bool\n    }\n\n    class RunContext {\n        -Metta metta\n        -ModId mod_id\n        -ModuleInitState init_state\n        +init_self_module()\n        +load_module() ModId\n        +import_all_from_dependency() Result\n    }\n\n    MettaMod --&gt; ModuleSpace : contains\n    MettaMod --&gt; ModuleLoader : uses\n    RunContext --&gt; MettaMod : manages\n    ModuleLoader --&gt; RunContext : interacts with</code></pre>"},{"location":"module-system/#module-loading-and-initialization-flow","title":"Module Loading and Initialization Flow","text":"<p>The module loading process follows a recursive pattern that enables complex dependency graphs and emergent module interactions.</p> <pre><code>sequenceDiagram\n    participant C as Client Code\n    participant M as Metta Runner\n    participant RS as RunnerState\n    participant RC as RunContext\n    participant MIS as ModuleInitState\n    participant ML as ModuleLoader\n    participant MM as MettaMod\n\n    C-&gt;&gt;M: load_module_direct(loader, \"module_name\")\n    M-&gt;&gt;RS: new_with_module(TOP)\n    RS-&gt;&gt;RC: run_in_context()\n    RC-&gt;&gt;RC: normalize_module_name()\n    RC-&gt;&gt;MIS: init_module()\n\n    Note over MIS: Create new ModuleInitFrame\n    MIS-&gt;&gt;RS: new_for_loading()\n    RS-&gt;&gt;RC: run_in_context(loader.load)\n    RC-&gt;&gt;ML: load(context)\n\n    Note over ML: Module-specific loading logic\n    ML-&gt;&gt;RC: init_self_module(space, resource_dir)\n    RC-&gt;&gt;MM: new MettaMod instance\n    ML-&gt;&gt;RC: push_parser(MeTTa code)\n\n    Note over RC: Execute module initialization\n    RC-&gt;&gt;RC: run_inline()\n    RC-&gt;&gt;RC: step() loop until complete\n\n    Note over RS: Module loading complete\n    RS-&gt;&gt;MIS: finalize_loading()\n    MIS-&gt;&gt;M: merge_init_state()\n    M-&gt;&gt;M: add_module_to_name_tree()\n    M-&gt;&gt;C: return ModId</code></pre>"},{"location":"module-system/#recursive-dependency-resolution","title":"Recursive Dependency Resolution","text":"<p>The system implements sophisticated dependency resolution with support for version constraints and hierarchical module spaces.</p> <pre><code>flowchart TD\n    A[Module Import Request] --&gt; B{Module Already Loaded?}\n    B --&gt;|Yes| C[Return Existing ModId]\n    B --&gt;|No| D[Parse Module Path]\n\n    D --&gt; E[Load Parent Modules]\n    E --&gt; F{Parent Exists?}\n    F --&gt;|No| G[Recursive Parent Loading]\n    G --&gt; E\n    F --&gt;|Yes| H[Resolve in Parent Context]\n\n    H --&gt; I[Check Package Info]\n    I --&gt; J[Apply Version Constraints]\n    J --&gt; K[Query Catalogs]\n\n    K --&gt; L{Module Found?}\n    L --&gt;|No| M[Resolution Error]\n    L --&gt;|Yes| N[Check Descriptor Cache]\n\n    N --&gt; O{Already Loaded?}\n    O --&gt;|Yes| P[Create Alias]\n    O --&gt;|No| Q[Initialize New Module]\n\n    Q --&gt; R[Recursive Dependency Load]\n    R --&gt; S[Module Integration]\n    S --&gt; T[Update Name Tree]\n    T --&gt; U[Return ModId]\n\n    P --&gt; U\n    C --&gt; U\n\n    style A fill:#e1f5fe\n    style E fill:#e8f5e8\n    style H fill:#fff3e0\n    style Q fill:#fce4ec\n    style R fill:#f3e5f5</code></pre>"},{"location":"module-system/#module-namespace-and-scoping","title":"Module Namespace and Scoping","text":"<p>The module system implements hierarchical namespacing with adaptive scope resolution and emergent visibility patterns.</p> <pre><code>stateDiagram-v2\n    [*] --&gt; TopLevel: Module Request\n    TopLevel --&gt; NameParsing: Parse Module Path\n    NameParsing --&gt; RelativeResolution: Relative Path\n    NameParsing --&gt; AbsoluteResolution: Absolute Path\n\n    RelativeResolution --&gt; ContextLookup: Current Module Context\n    ContextLookup --&gt; ParentTraversal: Search Parent Scopes\n    ParentTraversal --&gt; ContextLookup: Not Found\n    ParentTraversal --&gt; ModuleLocation: Found\n\n    AbsoluteResolution --&gt; GlobalNamespace: From Root\n    GlobalNamespace --&gt; ModuleLocation: Direct Lookup\n\n    ModuleLocation --&gt; VisibilityCheck: Check Access\n    VisibilityCheck --&gt; AccessGranted: Public/Interface\n    VisibilityCheck --&gt; AccessDenied: Private/Internal\n\n    AccessGranted --&gt; ModuleBinding: Create Reference\n    ModuleBinding --&gt; [*]: Success\n\n    AccessDenied --&gt; [*]: Error\n\n    note right of ParentTraversal\n        Hierarchical scope traversal:\n        - Current module scope\n        - Parent module scope  \n        - Transitive parent scopes\n        - Global namespace\n    end note\n\n    note right of VisibilityCheck\n        Visibility qualifiers:\n        - Public: Available to all\n        - Interface: Exported symbols\n        - Private: Internal only\n        - Implementation: Hidden details\n    end note</code></pre>"},{"location":"module-system/#package-management-integration","title":"Package Management Integration","text":"<p>The module system integrates with package management for distributed module loading and version resolution.</p> <pre><code>graph LR\n    A[Module Request] --&gt; B[PkgInfo Resolution]\n    B --&gt; C[Version Constraints]\n    C --&gt; D[Catalog Query]\n\n    D --&gt; E[Local Cache]\n    D --&gt; F[Remote Repositories]\n    D --&gt; G[File System]\n\n    E --&gt; H{Cache Hit?}\n    H --&gt;|Yes| I[Load from Cache]\n    H --&gt;|No| J[Fetch Required]\n\n    F --&gt; K[Network Fetch]\n    K --&gt; L[Version Validation]\n    L --&gt; M[Security Check]\n    M --&gt; N[Cache Update]\n\n    G --&gt; O[Local Module Discovery]\n    O --&gt; P[Path Resolution]\n    P --&gt; Q[Format Detection]\n\n    I --&gt; R[Module Preparation]\n    N --&gt; R\n    Q --&gt; R\n\n    R --&gt; S[Dependency Analysis]\n    S --&gt; T[Recursive Loading]\n    T --&gt; U[Module Integration]\n\n    style A fill:#e1f5fe\n    style B fill:#e8f5e8\n    style D fill:#fff3e0\n    style R fill:#fce4ec\n    style T fill:#f3e5f5</code></pre>"},{"location":"module-system/#module-import-patterns-and-strategies","title":"Module Import Patterns and Strategies","text":"<p>Different import patterns enable various levels of cognitive integration and namespace management.</p> <pre><code>flowchart TD\n    A[Import Request] --&gt; B{Import Type}\n\n    B --&gt;|import_all| C[Complete Module Import]\n    B --&gt;|import_as| D[Selective Import]\n    B --&gt;|import_item| E[Specific Item Import]\n\n    C --&gt; F[Deep Copy Module Space]\n    F --&gt; G[Strip Transitive Dependencies]\n    G --&gt; H[Add as Grounded Atom]\n    H --&gt; I[Merge Tokenizer Entries]\n    I --&gt; J[Complete Integration]\n\n    D --&gt; K[Dependency Registration]\n    K --&gt; L[Tokenizer Entry Creation]\n    L --&gt; M[Namespace Binding]\n    M --&gt; N[Controlled Access]\n\n    E --&gt; O[Item Resolution]\n    O --&gt; P{Tokenizer Entry?}\n    P --&gt;|Yes| Q[Import Token Pattern]\n    P --&gt;|No| R[Resolve as Atom]\n\n    Q --&gt; S[Direct Token Import]\n    R --&gt; T[Atom Resolution]\n    T --&gt; U[Optional Renaming]\n    U --&gt; V[Targeted Integration]\n\n    J --&gt; W[Emergent Behavior]\n    N --&gt; W\n    S --&gt; W\n    V --&gt; W\n\n    style A fill:#e1f5fe\n    style C fill:#e8f5e8\n    style D fill:#fff3e0\n    style E fill:#fce4ec\n    style W fill:#f3e5f5</code></pre>"},{"location":"module-system/#cognitive-module-interaction-patterns","title":"Cognitive Module Interaction Patterns","text":"<p>Modules interact through cognitive patterns that enable emergent behavior and adaptive learning.</p> <pre><code>mindmap\n  root((Module Interactions))\n    Space Integration\n      Hierarchical Spaces\n        Base Module Space\n        Imported Dependency Spaces\n        Transitive Space Access\n      Query Propagation\n        Local Space Query\n        Dependency Space Search\n        Result Aggregation\n    Token Sharing\n      Context-Independent Tokens\n        Mathematical Operations\n        Logical Operators\n        Basic Data Types\n      Context-Dependent Tokens\n        Module-Specific Functions\n        Domain Operations\n        Cognitive Primitives\n    Dependency Patterns\n      Import All\n        Complete Cognitive Integration\n        Emergent Behavior Patterns\n        Namespace Pollution Risk\n      Selective Import\n        Controlled Integration\n        Explicit Dependencies\n        Clean Interfaces\n      Item Import\n        Precise Control\n        Minimal Footprint\n        Targeted Enhancement\n    Cognitive Synergy\n      Pattern Recognition\n        Cross-Module Patterns\n        Emergent Relationships\n        Adaptive Learning\n      Knowledge Integration\n        Symbolic Reasoning\n        Neural Processing\n        Hybrid Cognition</code></pre>"},{"location":"module-system/#module-resource-management","title":"Module Resource Management","text":"<p>Each module can provide resources that extend beyond executable code, supporting rich cognitive environments.</p> <pre><code>sequenceDiagram\n    participant C as Client\n    participant RC as RunContext\n    participant MM as MettaMod\n    participant ML as ModuleLoader\n    participant FS as File System\n\n    C-&gt;&gt;RC: load_resource_from_module(mod_name, resource_key)\n    RC-&gt;&gt;RC: get_module_by_name(mod_name)\n\n    alt Module Already Loaded\n        RC-&gt;&gt;MM: get_resource(resource_key)\n        MM-&gt;&gt;ML: get_resource(resource_key)\n        ML-&gt;&gt;FS: load resource file\n        FS--&gt;&gt;ML: resource data\n        ML--&gt;&gt;MM: Resource object\n        MM--&gt;&gt;RC: Resource object\n    else Module Not Loaded\n        RC-&gt;&gt;RC: resolve_module(mod_name)\n        RC-&gt;&gt;ML: new ModuleLoader\n        ML-&gt;&gt;ML: get_resource(resource_key)\n        ML-&gt;&gt;FS: load resource file\n        FS--&gt;&gt;ML: resource data\n        ML--&gt;&gt;RC: Resource object\n    end\n\n    RC--&gt;&gt;C: Resource object\n\n    Note over ML, FS: Resources can include:&lt;br/&gt;- Data files&lt;br/&gt;- Model weights&lt;br/&gt;- Configuration&lt;br/&gt;- Documentation&lt;br/&gt;- Binary assets</code></pre> <p>This module system architecture enables sophisticated cognitive applications through hierarchical organization, recursive dependency resolution, and adaptive integration patterns that support both symbolic reasoning and neural processing capabilities.</p>"},{"location":"modules_dev/","title":"MeTTa Modules (Rust / Python Developer Documentation)","text":"<p>TODO: Integrate this documentation within the larger MeTTa Book</p> <p>Modules are implementations of free-standing MeTTa functionality that can be imported into other MeTTa modules or programs.  Modules may be implemented in MeTTa code itself, but they may also include functionality implemented with, or linked from host languages such as Rust, C, or Python.  Modules may include additional files and resources as well.</p> <p>NOTE: Importantly, a module can have sub-module dependencies, aka \"downward\" dependencies, but it cannot have \"upward\" dependencies, ie. dependencies on the client code importing the module.</p>"},{"location":"modules_dev/#what-is-a-module","title":"What is a Module?","text":"<p>Fundamentally a module in a persistent encapsulation of a context within which MeTTa code can run.  Every module has a unique [Space] (and also a [Tokenizer], for now).  For MeTTa code running within the context of a module, the <code>&amp;self</code> token will resolve to the module's space.</p> <p>A loaded module is represented with the [MettaMod] struct.  In addition to a [Space] and a [Tokenizer], a module may also contain a filesystem path to the module's resources, the sub-modules imported by the module, a [ModuleDescriptor] object, and a [PkgInfo] struct, the latter of which are documented in the Package Management section.</p> <p>To execute code in the context of any loaded module, use the [RunnerState::new_with_module] method.</p>"},{"location":"modules_dev/#loading-a-module","title":"Loading a Module","text":"<p>Modules are loaded into a [Metta] runner using one of the module loading methods: [Metta::load_module_direct], [Metta::load_module_at_path], or [RunContext::load_module].  Loaded modules are referred to with a [ModId].</p> <p>Fundamentally, all modules are loaded via a loader object that implements the [ModuleLoader] trait.  Irrespective of the module's original format or host language, a loader object's [ModuleLoader::load] function ultimately loads the module into the runner.</p>"},{"location":"modules_dev/#module-names-name-paths","title":"Module Names &amp; Name Paths","text":"<p>Each loaded module must have a name.  A legal module name is an ascii string, containing only alpha-numeric characters plus <code>_</code> and <code>-</code>.</p> <p>If module loading is initiated through the MeTTa <code>import!</code> operation or the corresponding [RunContext::load_module] API call, then the module name will be used to identify the module to load, following the logic in the Module Name Resolution section.</p> <p>Direct module-loading API calls such as [Metta::load_module_direct], [Metta::load_module_at_path], or [Metta::load_module_alias] all take an explicit module name or name path.</p> <p>Upon loading, the module is subsequently placed into the module name path hierarchy, where <code>top</code> is always the name for the top-module in the runner and the <code>':'</code> character acts as the separator.  An example module name path looks like <code>top:mod1:sub_a</code>, and an example hierarchy is illustrated below.</p> <pre><code>top = 0\n \u251c\u2500corelib = 1\n \u251c\u2500stdlib = 2\n \u251c\u2500mod1 = 3\n \u2502  \u2514\u2500sub_a = 4\n \u2514\u2500mod2 = 5\n    \u2514\u2500sub_b = 6\n</code></pre> <p>In addition, the <code>self</code> token may be used at the beginning of a module name path to refer to the currently running module context.  In the context of the top module, <code>top</code> and <code>self</code> should have an identical meaning.</p> <p>Step-by-step, the MeTTa code: <pre><code>!(import! &amp;self some_module)\n</code></pre> will cause the name <code>some_module</code> to be resolved into a specific module instance; if that module is not yet loaded then it will be, and finally the module will be imported, in totality, into the currently executing module (context).</p> <p></p> <p>NOTE: The same loaded module (with the same ModId) may appear multiple times in the hierarchy, sometimes with different names.  This could be the effect of an \"import as\" operation or making a module alias.</p> <p>NOTE: The same module name may occur in multiple places in the hierarchy, and there is no guaranteed a name will always refer to the same module.  However, within a given node of the module name hierarchy, a module name will always be unique.</p>"},{"location":"modules_dev/#importing-a-module","title":"Importing a Module","text":"<p>A module is imported into another module using one of the import methods:  - [MettaMod::import_dependency_as], corresponding to <code>import module as name</code>  - [MettaMod::import_all_from_dependency], corresponding to <code>import * from module</code>  - [MettaMod::import_item_from_dependency_as], corresponding to <code>import item from module as name</code></p> <p>Once imported, a sub-module is accessed via an embedded [Space] atom in the destination module's space, [Tokenizer] entries for accessing the source module's space, tokens, or a combination of the two.</p>"},{"location":"modules_dev/#behavior-wip","title":"Behavior WIP","text":"<p>TODO: The precise semantics of importing (in other words, linking) are still under discussion and development.  Specifically we may wish to provide a mechanism to explicitly declare what is exported from a module (and thus available for import).  This would be similar to the <code>export</code> key words in some languages, or the <code>pub</code> visibility qualifiers in Rust.</p> <p>In addition, some changes will be needed so that [Tokenizer] entries can be imported and accessed between modules.  Currently Tokenizer entries are only imported using the [MettaMod::import_all_from_dependency] method, and the mechanism at work may lead to unreachable Tokenizer entries.</p> <p>Some issues regarding this are: [https://github.com/trueagi-io/hyperon-experimental/issues/509] [https://github.com/trueagi-io/hyperon-experimental/issues/511] [https://github.com/trueagi-io/hyperon-experimental/issues/510]</p> <p>More discussion on these topics is in the section: \"Importing / Linking\" of the <code>modules_internal_discussion.md</code> file.</p>"},{"location":"modules_dev/#package-management","title":"Package Management","text":"<p>Package Management is the set of features that allow for: - searching for modules across multiple locations (Catalogs) - expressing version requirements and selecting compatible versions - loading modules from files or other locations</p> <p>Modularity is a fundamental and inseparable aspect of the MeTTa runner, but Package Management features could be optional.</p>"},{"location":"modules_dev/#module-file-formats","title":"Module File Formats","text":"<p>Modules may be loaded from files and other file-system-like resources (for example, a github repo) using the objects that implement the [FsModuleFormat] trait.  This trait contains the interface to interpret a file or a directory and instantiate a [ModuleLoader] to load the module(s) contained within.</p> <p>The objects [SingleFileModuleFmt] and [DirModuleFmt] are part of the default environment and are capable of loading MeTTa modules from single <code>.metta</code> files and directories containing a <code>module.metta</code> file respectively.  Additionally, the <code>hyperon</code> Python module contains a [FsModuleFormat] for loading MeTTa modules from Python modules - both stand-alone <code>.py</code> files as well as directories containing an <code>__init__.py</code> file.</p> <p>More information on the individual module file formats is available in the MeTTa usage documentation and MeTTa Python documentation respectively.</p>"},{"location":"modules_dev/#the-pkginfo-structure","title":"The PkgInfo Structure","text":"<p>Each module has an associated [PkgInfo] structure, which provides the module author a place to specify meta-data about the module and express requirements for the module's dependencies.  Additionally a [PkgInfo] can provide explicit loading instructions such as file system paths or github URLs for dependent modules.  The [PkgInfo] structure is the same concept as the Cargo.toml file used in Cargo/Rust.</p> <p>The [PkgInfo] should be initialized inside the module's loader function.  If it is not initialized then default values will be used.</p> <p>The fields of the [PkgInfo] struct are documented in the Rust MeTTa documentation here.</p> <p>TODO: PkgInfo documentation also belongs in user-facing docs.  In that section, cover how to specify the pkginfo as a MeTTa structure and/or in a <code>_pkg-info.metta</code> or <code>_pkg-info.json</code> file as opposed to as a Rust struct.</p>"},{"location":"modules_dev/#module-name-resolution","title":"Module Name Resolution","text":"<p>When MeTTa code executes the <code>!(import! &amp;space some_module)</code> operation, the module name needs to be mapped to a loaded or loadable module.  This process occurs according to the logic described by the flowchart below.</p> <p></p> <ol> <li> <p>First the module name is checked against the modules which are already loaded within the context of the running module.  This ensures the same instance of a shared dependency will be loaded everywhere.</p> </li> <li> <p>If a loaded module is not available, the [PkgInfo] will be checked for a corresponding entry.  If an entry specifies a specific location in the file system or a remote repository, then the module will be loaded from that location.  Additionally, the [PkgInfo] may specify version requirements for use by the catalog in locating and selecting an accaptable module.</p> </li> <li> <p>Finally, the Catalogs from the Environment will be queried in priority order. (See the Catalogs section below)</p> </li> </ol> <p>By default, the built-in search paths / catalogs are:</p> <ol> <li>The module's own <code>resource</code> directory, if it has one</li> <li>The <code>hyperon/exts/</code> directory, if the Hyperon Python module is running</li> <li>The MeTTa config <code>modules</code> directory, at an OS-specific location.</li> </ol> <p>Depending on the host OS, the config directory locations will be: * Linux: ~/.config/metta/ * Windows: ~\\AppData\\Roaming\\TrueAGI\\metta\\config\\ * Mac: ~/Library/Application Support/io.TrueAGI.metta/</p> <p>In the future we may create a centralized module catalog along the lines of <code>PyPI</code> or <code>crates.io</code>.</p>"},{"location":"modules_dev/#catalogs","title":"Catalogs","text":"<p>An object that implements the [ModuleCatalog] trait exposes an interface to locate modules based on name and version constraints, and create [ModuleLoader] objects to retrieve and load those modules.</p> <p>One built-in [ModuleCatalog] object type is the [DirCatalog].  As described in the \"Module File Formats\" section, a [DirCatalog] uses a collection of [FsModuleFormat] objects to export a catalog of modules contained within its associated directory.</p> <p>Additional catalogs may be implemented for other module repository formats or protocols - for example a central package service similar to <code>PyPI</code> or <code>crates.io</code>, as mentioned earlier.</p>"},{"location":"modules_dev/#implementing-a-moduleloader","title":"Implementing a [ModuleLoader]","text":"<p>All modules are ultimately loaded programmatically through the MeTTa API, and it's the role of a [ModuleLoader] to make the necessary API calls.</p> <p>The [ModuleLoader::load] method ultimately sets up the module.  Each module has its own [Space] so the space needs to be created first.  Then the module must be initialized using the [RunContext::init_self_module] method.</p> <p>After <code>init_self_module</code> has run, it is now legal to access the module data stricture using [RunContext::module] or [RunContext::module_mut], as well as enqueuing MeTTa code or additional operations to run.</p> <p>An example <code>load</code> method implementation is here: <pre><code>fn load(&amp;self, context: &amp;mut RunContext, descriptor: ModuleDescriptor) -&gt; Result&lt;(), String&gt; {\n\n    let space = DynSpace::new(GroundingSpace::new());\n    let resource_dir = std::path::PathBuf::from(\"/tmp/test_module_resources\")\n    context.init_self_module(descriptor, space, Some(resource_dir.into()));\n\n    let parser = SExprParser::new(METTA_PROGRAM_TEXT);\n    context.push_parser(Box::new(parser));\n\n    Ok(())\n}\n</code></pre></p>"},{"location":"modules_internal_discussion/","title":"Module Design Discussion - Open Questions","text":""},{"location":"modules_internal_discussion/#overview-what-does-module-mean","title":"Overview: What does Module mean?","text":"<p>When I began this task, I conflated the design-space of modularity with package management, however the two are separate and distinct.  I began the task by designing and implementing a package manager, but the current MeTTa semantics lack the fundamental modularity constructs that are a prerequisite for robust package management.</p> <p>It may seem like a tautology, but I feel it bears repeating that the purpose of modules is modularity.  In other words, isolating the implementation and incidental behaviors of some \"module\" code from other \"client\" code.</p> <p>Definition: A module is unit of functionality that can be loaded into a MeTTa runner and used by other MeTTa code without requiring modification of the module, regardless of the location or the format from which the module was loaded.</p>"},{"location":"modules_internal_discussion/#desiderata","title":"Desiderata","text":"<ul> <li>A module should be able to be loaded and tested independently of client code that might load it</li> <li>A module should express its own sub-module dependencies</li> <li>A module should not rely on the client to import sub-dependencies or configure the environment for the module (\"environment\" used here in a loose sense, not the Environment object)</li> <li>A corollary to the above is that a module should not be affected by things the client does aside from the interactions through the module's interface.  For example, a client shouldn't be able to accidentally break a module's implementation by defining atoms or tokens in the client code. </li> </ul> <p>By analogy, Rust <code>mod</code>s meet these criteria, and C header files do not.  Shared libraries, on the other hand, are modules according to the criteria above.</p>"},{"location":"modules_internal_discussion/#mettamod-struct","title":"MettaMod struct","text":"<p>The [MettaMod] struct essentially takes the place of a \"Loading Runner\" in the prior code base.  The major difference being that the MettaMod continues to exist even after the module loader code has evaluated.  More discussion can be found in the \"What is a Module?\" section of the main module docs.</p>"},{"location":"modules_internal_discussion/#question-topic-1-importing-linking","title":"Question Topic 1: Importing &amp; Linking","text":"<p>To discuss import behavior, we need to discuss 3 distinct cases separately.  The cases are:</p> <ul> <li>Import As <code>import module as name</code></li> <li>Import All (aka import *) <code>import * from module</code></li> <li>Import Item <code>import item from module as name</code></li> </ul> <p>In parallel, a discussion on this topic exists here: https://github.com/trueagi-io/hyperon-experimental/issues/509 although this document presently represents a more thorough treatment of the topic.</p>"},{"location":"modules_internal_discussion/#import-as","title":"Import As","text":"<p>This is probably the most straightforward of the three import behaviors.  Currently the code registers a new Tokenizer entry in the destination module's Tokenizer with the space of sub-module.  This behavior is adequate and does the right thing for allowing the sub-module's space to be accessed within the context of the client module.</p> <p>This is implemented by the [MettaMod::import_dependency_as] method.</p> <p>Outstanding Issue: This implementation doesn't provide any way to access tokenizer entries that are part of the sub-module.</p>"},{"location":"modules_internal_discussion/#import-all","title":"Import All","text":"<p>Conceptually this operation imports the entire contents of a sub-module into the target module.  Another way to think about this is overlaying the space and tokens of the sub-module onto the client module.  In practice, given the behavior of the interpreter, the current implementation is a bit convoluted.</p> <p>Currently the implementation leverages the behavior that a nested Space Grounded Atom will behave as an extension of the Space containing it.</p> <p>The import code makes a deep-copy of the sub-module's space, then strips away space atoms that are associated with dependency's own sub-modules (2<sup>nd</sup> order sub-modules), before finally adding the cloned sub-space as a grounded atom into the client's space.</p> <p>After that, the implementation imports the all of the new secondary transitive dependencies into the client, and finally it merges all tokenizer entries from the sub-module into the client.</p> <p>This \"import all\" behavior is implemented by the [MettaMod::import_all_from_dependency] method.</p>"},{"location":"modules_internal_discussion/#this-current-implementation-some-issues","title":"This current implementation some issues","text":"<ul> <li> <p>Inefficient Space Clone: Deep-cloning the sub-modules's space on every import is costly, considering it should be possible to import functionality by reference.  This potentially leads to many copies of the same sub-modules in memory and defeats one of the benefits of modularity.  One potential way to fix this it to create a [Space] implementation that I'll call <code>ModuleSpace</code>, which wraps the module's <code>&amp;self</code> space regardless of its underlying type.  <code>ModuleSpace</code> holds references to the sub-modules' individual spaces and controls which how the sub-module spaces will be combined.  I have not yet tried to implement this so I don't know what other problems may come up.</p> </li> <li> <p>Private Sub-Modules shouldn't be lifted: All transitive dependency sub-modules are lifted into the client scope, including private sub-modules.  Assuming we have a fix for the above, addressing this point may be as simple as not stripping and lifting private sub-modules, so that may be an easy fix.</p> </li> <li> <p>Redundant Tokenizer Entries: Currently Tokenizer entries imported from sub-modules may be inaccessible because they are superseded by existing entries in the module's Tokenizer.  A partial solution here might be to implement something like a \"Layered Tokenizer\" as described here: https://github.com/trueagi-io/hyperon-experimental/issues/408#issuecomment-1839196513</p> </li> </ul>"},{"location":"modules_internal_discussion/#explicit-exports-or-visibility-qualifiers","title":"Explicit exports or visibility qualifiers","text":"<p>Most languages that support modularity allow a module to declare a subset of the objects as available for export.  Sometimes this is done with visibility qualifiers, (for example <code>pub</code> in Rust), or in other situations this is accomplished via the ABI and header files, as with shared libraries imported in C &amp; C++.</p> <p>Explicit control over exports would solve the problem of duplicated or conflicting transitive imports of sub-modules, because private sub-modules would not be exported.</p>"},{"location":"modules_internal_discussion/#import-item","title":"Import Item","text":"<p>The <code>import item from module [as name]</code> is implemented with the [MettaMod::import_item_from_dependency_as] method.  However there is no <code>stdlib</code> operation that calls it, so it's currently inaccessible from MeTTa code.</p>"},{"location":"modules_internal_discussion/#tokenizer-entries-dont-always-have-names-to-import","title":"Tokenizer entries don't always have names to import","text":"<p>When we want to import a tokenizer entry from a sub-module, we need a name to refer to it.  But currently Tokenizer entries don't have names.  By itself, this problem is easy to address, but we should consider our choice of solution in light of the other choices we need to make for the MeTTa language.</p> <p>This issue is discussed in more detail here: https://github.com/trueagi-io/hyperon-experimental/issues/510</p> <p>One solution, as described on github, is to require Tokenizer entries to have corresponding items in the module's Space.</p>"},{"location":"modules_internal_discussion/#question-topic-2-sub-module-version-resolution-discussion","title":"Question Topic 2: Sub-Module Version Resolution Discussion","text":""},{"location":"modules_internal_discussion/#background","title":"Background","text":"<p>There are two desiderata which are at odds with each other and must be balanced:</p> <ul> <li> <p>Implementation sub-modules: We want sub-module dependencies not to interfere with each other as much as is possible.  For example, ModA should be able to import ModC, and ModB should be able to import a different version of ModC.  This should be allowed as long as both ModA and ModB use ModC within their internal implementation.  Within this document I will call this pattern an \"Implementation\" sub-module.</p> </li> <li> <p>Interface sub-modules If ModA imports ModC and uses functionality from ModC in its interface, then ClientMod will transitively import ModC when it imports ModA.  In effect this means the dependent sub-module's version becomes part of the module's interface.  Therefore, within this document, I will call this an \"Interface\" sub-module.  If ClientMod imports ModA and ModB, which both import ModC as an Interface sub-module, then the version (instance) of ModC must be the same between ModA and ModB.</p> </li> </ul>"},{"location":"modules_internal_discussion/#sub-module-set-satisfiability","title":"Sub-Module set satisfiability","text":"<p>The versions of transitive Interface sub-modules must be reconciled such that a given client imports exactly one version of each dependent sub-module, even when that sub-module is shared between two or more other dependents.  Finding the compatible module versions is a flavor of the Satisfiability Problem.</p> <p>Reasonably sophisticated package managers such as Cargo include a Satisfiability Solver, which will attempt to find a set of sub-module versions that satisfy the requirements for each client that imports each dependency.  I believe we will ultimately want to add a solver to MeTTa too, someday.  However this doesn't currently exist.  MeTTa as a language is well-suited to implementing a solver.</p> <p>Until we add a solver to the MeTTa Package Manager, the module resolution logic will progress sequentially with no ability to backtrack.  This means that we will need to rely on each module's author to manually determine a workable set of sub-module versions for themselves.  Then the specific (or narrower) version requirements can be added to the module's [PkgInfo].</p> <p>Current State of the Code: Version requirements for sub-modules are not yet implemented so this is a non-issue in the present implementation.</p>"},{"location":"modules_internal_discussion/#module-namespace-scope","title":"Module Namespace Scope","text":"<p>When a module is loaded, the module's name may be registered in the runner's module namespace, so that subsequent attempts to resolve the same module name elsewhere in the runner will return the already-loaded module as opposed to causing a new instance of the module to be loaded.</p> <p>The question here is which behavior we want for the module namespace:</p> <ul> <li> <p>Proposal A: Global Scope:  This means every module is effectively an Interface Module, and only one version of a given module can be loaded in the same runner.  This is the approach taken by both Cargo and Python (but for different reasons in each case), and a number of other package managers.  However, the reasons that have driven other package managers towards this design don't necessarily apply to MeTTa, and I believe we can support private Implementation sub-modules.  Therefore I do not advocate for this design.</p> </li> <li> <p>Proposal B: Conservative Hierarchical Scope:  Within this design, each module name only propagates upwards to its client if it needs to, or is explicitly re-exported.  This means that Interface modules must be exported or marked some other way.  It also complicates the Sat Solver (see above) as it would need to operate over multiple sets with some partial intersections between them.  I believe this is probably the \"right\" design, but also the most work to implement.</p> </li> <li> <p>Proposal C: 2-Layer \"Public vs. Private\" Scope:  This is the way the code currently works.  Each imported module may be \"Public\", in which case it is available by name to all other modules in the runner, or it may be \"Private\", in which case it is loaded only for the client module that resolves it.  This is a \"poor man's\" version of Proposal B.  However, it has many problematic edge cases so I would prefer to implement Proposal B.</p> </li> </ul> <p>For either Proposal B or C, we probably want to add some form of linting / checking, so users don't accidentally shoot themselves in the foot by exporting items that depend on private (Implementation) sub-modules.  In other words, public interfaces must not include non-public objects.</p> <p>UPDATE: A version of B is now implemented in the code, with the caveat that nothing is private (visibility constructs don't exist for MeTTa modules at all) but sub-modules can be contained within the namespace of their respective parents.</p>"},{"location":"modules_internal_discussion/#remaining-implementation-work","title":"Remaining implementation work","text":"<p>LP-TODO, search code for remaining TODO comments and collect them here so I can prioritize remaining work</p>"},{"location":"modules_internal_discussion/#moduledescriptor-should-be-part-of-pkg_management-features","title":"ModuleDescriptor should be part of pkg_management features","text":"<p>However, we still need a way to globally (within a runner) disambiguate a module.  Currently that's done with a ModuleDescriptor.  My proposal is to implement the hierarchical module name-space.  It's pretty clear that's the direction we want to go, as several open-issues are pointing that way.</p> <p>Then the loaded module table will use full module path instead of ModuleDescriptor, and ModuleDescriptor will be only for modules in a catalog that are not yet loaded.</p>"},{"location":"modules_internal_discussion/#try-stripping-but-not-lifting-private-transitive-imports-in-import_all","title":"Try stripping but not lifting private transitive imports in <code>import_all</code>","text":"<p>In response to the \"Private Sub-Modules shouldn't be lifted\" section, in a discussino with Vitaly I realized perhaps all of the necessary interfaces to transitive sub-modules might be acomplished within the sub-module's space.  So therefore we may be able to get away with stripping but not lifting the private dependencies.</p>"},{"location":"modules_internal_discussion/#catalog-for-all-python-modules","title":"Catalog for All Python modules","text":"<p>Need to implement a ModuleCatalog that publishes all Python modules as MeTTa modules.  This will achieve parity with the current implementation and re-enable the practice of using <code>pip</code> to install MeTTa modules.  Additionally, we may want to support hierarchical module path imports, so that the MeTTa import operation is able to traverse the python module hierarchy to load and import nested Python modules.</p>"},{"location":"modules_internal_discussion/#atom-serde-serde-format-to-encode-any-structured-data-into-atoms","title":"\"atom-serde\", serde format to encode any structured data into atoms","text":"<p>This relates to package management because we want to represent a PkgInfo structure as atoms within MeTTa code.  One reusable appraoch to this is to make a serde format that targets atoms.  This can then be leveraged for many API structures such as the parse tree, etc.</p> <p>There is a discussion on the topic here: https://github.com/trueagi-io/hyperon-experimental/issues/455</p> <p>Nil's work here https://github.com/trueagi-io/protobuf-metta and more generally this thread https://chat.singularitynet.io/chat/pl/u8u9jzrnmp85d8ounmpbmjukyc has raised the idea that we pursue a format that relies on a structure definition that uses the MeTTa type-system, rather than trying to create a self-describing format.</p>"},{"location":"modules_internal_discussion/#circular-import-testing-guard-rails","title":"Circular-import testing &amp; guard-rails","text":"<p>I need to make sure the right things happen when circular loading / imports are attempted.  More dicussion here: https://github.com/trueagi-io/hyperon-experimental/pull/580#discussion_r1491178245</p>"},{"location":"space-tokenizer/","title":"Space and Tokenizer Architecture","text":"<p>The Hyperon space and tokenizer systems form the cognitive foundation for symbolic reasoning, neural-symbolic integration, and adaptive attention allocation through hierarchical knowledge representation and context-aware parsing.</p>"},{"location":"space-tokenizer/#space-system-architecture","title":"Space System Architecture","text":"<p>The space system implements a multi-layered architecture that enables emergent cognitive patterns through hierarchical knowledge organization and adaptive query processing.</p> <pre><code>graph TD\n    A[DynSpace Interface] --&gt; B[Space Implementations]\n    B --&gt; C[GroundingSpace]\n    B --&gt; D[ModuleSpace] \n    B --&gt; E[Custom Space Types]\n\n    C --&gt; F[Atom Storage]\n    C --&gt; G[Query Engine]\n    C --&gt; H[Index Structures]\n\n    D --&gt; I[Base Space]\n    D --&gt; J[Imported Dependencies]\n    D --&gt; K[Transitive Access]\n\n    F --&gt; L[Atom Collections]\n    F --&gt; M[Type Indexing]\n    F --&gt; N[Pattern Matching]\n\n    G --&gt; O[Unification Engine]\n    G --&gt; P[Binding Resolution]\n    G --&gt; Q[Result Composition]\n\n    I --&gt; R[Module Atoms]\n    J --&gt; S[Dependency Spaces]\n    K --&gt; T[Recursive Queries]\n\n    style A fill:#e1f5fe\n    style C fill:#e8f5e8\n    style D fill:#fff3e0\n    style G fill:#fce4ec\n    style O fill:#f3e5f5</code></pre>"},{"location":"space-tokenizer/#space-query-and-unification-patterns","title":"Space Query and Unification Patterns","text":"<p>The space system implements sophisticated query processing with recursive unification and emergent pattern recognition.</p> <pre><code>sequenceDiagram\n    participant C as Client\n    participant MS as ModuleSpace\n    participant BS as Base Space\n    participant DS1 as Dependency Space 1\n    participant DS2 as Dependency Space 2\n    participant UE as Unification Engine\n\n    C-&gt;&gt;MS: query(pattern)\n    MS-&gt;&gt;BS: query_local(pattern)\n    BS-&gt;&gt;UE: unify_atoms(pattern, candidates)\n    UE--&gt;&gt;BS: local_bindings\n    BS--&gt;&gt;MS: local_results\n\n    alt Local Results Insufficient\n        MS-&gt;&gt;DS1: query_dependency(pattern)\n        DS1-&gt;&gt;UE: unify_atoms(pattern, dep_candidates)\n        UE--&gt;&gt;DS1: dep_bindings_1\n        DS1--&gt;&gt;MS: dep_results_1\n\n        MS-&gt;&gt;DS2: query_dependency(pattern)\n        DS2-&gt;&gt;UE: unify_atoms(pattern, dep_candidates)\n        UE--&gt;&gt;DS2: dep_bindings_2\n        DS2--&gt;&gt;MS: dep_results_2\n    end\n\n    MS-&gt;&gt;MS: aggregate_results(local, dep1, dep2)\n    MS-&gt;&gt;MS: resolve_conflicts()\n    MS-&gt;&gt;MS: apply_cognitive_filters()\n    MS--&gt;&gt;C: unified_bindings_set\n\n    Note over MS: Emergent behavior through&lt;br/&gt;cross-space pattern recognition</code></pre>"},{"location":"space-tokenizer/#tokenizer-system-architecture","title":"Tokenizer System Architecture","text":"<p>The tokenizer system provides adaptive parsing and context-aware token resolution, enabling dynamic language evolution and cognitive pattern recognition.</p> <pre><code>graph TD\n    A[Tokenizer] --&gt; B[Token Registry]\n    A --&gt; C[Context Management]\n    A --&gt; D[Parse Engine]\n\n    B --&gt; E[Context-Independent Tokens]\n    B --&gt; F[Context-Dependent Tokens]\n    B --&gt; G[Function Tokens]\n\n    E --&gt; H[Mathematical Operators]\n    E --&gt; I[Logical Operators] \n    E --&gt; J[Basic Data Types]\n\n    F --&gt; K[Space-Dependent Ops]\n    F --&gt; L[Module-Specific Functions]\n    F --&gt; M[Cognitive Primitives]\n\n    G --&gt; N[Grounded Functions]\n    G --&gt; O[Custom Executables]\n    G --&gt; P[Extension Points]\n\n    C --&gt; Q[Current Module Context]\n    C --&gt; R[Space References]\n    C --&gt; S[Execution Environment]\n\n    D --&gt; T[Regex Matching]\n    D --&gt; U[Token Resolution]\n    D --&gt; V[Atom Construction]\n\n    style A fill:#e1f5fe\n    style B fill:#e8f5e8\n    style C fill:#fff3e0\n    style D fill:#fce4ec</code></pre>"},{"location":"space-tokenizer/#context-dependent-token-resolution","title":"Context-Dependent Token Resolution","text":"<p>The tokenizer implements sophisticated context resolution that enables adaptive behavior and emergent cognitive patterns.</p> <pre><code>flowchart TD\n    A[Parse Request] --&gt; B[Token Recognition]\n    B --&gt; C{Token Type}\n\n    C --&gt;|Static| D[Context-Independent]\n    C --&gt;|Dynamic| E[Context-Dependent]\n    C --&gt;|Function| F[Grounded Function]\n\n    D --&gt; G[Direct Resolution]\n    G --&gt; H[Static Atom Creation]\n\n    E --&gt; I[Context Analysis]\n    I --&gt; J[Current Module Space]\n    I --&gt; K[Current Tokenizer State]\n    I --&gt; L[Execution Environment]\n\n    J --&gt; M[Space-Dependent Resolution]\n    K --&gt; N[State-Dependent Resolution]\n    L --&gt; O[Environment-Dependent Resolution]\n\n    M --&gt; P[Dynamic Atom Creation]\n    N --&gt; P\n    O --&gt; P\n\n    F --&gt; Q[Function Registration Lookup]\n    Q --&gt; R[Parameter Type Checking]\n    R --&gt; S[Grounded Atom Creation]\n\n    H --&gt; T[Atom Result]\n    P --&gt; T\n    S --&gt; T\n\n    T --&gt; U[Cognitive Integration]\n    U --&gt; V[Emergent Behavior]\n\n    style A fill:#e1f5fe\n    style I fill:#e8f5e8\n    style U fill:#fff3e0\n    style V fill:#fce4ec</code></pre>"},{"location":"space-tokenizer/#neural-symbolic-integration-through-spaces","title":"Neural-Symbolic Integration through Spaces","text":"<p>The space system enables neural-symbolic integration through adaptive knowledge representation and emergent pattern recognition.</p> <pre><code>stateDiagram-v2\n    [*] --&gt; SymbolicInput: Symbolic Knowledge\n    [*] --&gt; NeuralInput: Neural Patterns\n    [*] --&gt; HybridInput: Hybrid Information\n\n    SymbolicInput --&gt; AtomRepresentation: Parse &amp; Structure\n    NeuralInput --&gt; EmbeddingSpace: Vector Representation\n    HybridInput --&gt; AdaptiveRepresentation: Dynamic Encoding\n\n    AtomRepresentation --&gt; SymbolicSpace: Store in Space\n    EmbeddingSpace --&gt; NeuralSpace: Neural Storage\n    AdaptiveRepresentation --&gt; HybridSpace: Adaptive Storage\n\n    SymbolicSpace --&gt; PatternRecognition: Query Patterns\n    NeuralSpace --&gt; SimilaritySearch: Vector Similarity\n    HybridSpace --&gt; CognitiveQuery: Multi-modal Query\n\n    PatternRecognition --&gt; SymbolicReasoning: Logical Inference\n    SimilaritySearch --&gt; NeuralProcessing: Neural Computation\n    CognitiveQuery --&gt; HybridReasoning: Integrated Processing\n\n    SymbolicReasoning --&gt; Integration: Symbolic Results\n    NeuralProcessing --&gt; Integration: Neural Results\n    HybridReasoning --&gt; Integration: Hybrid Results\n\n    Integration --&gt; EmergentCognition: Cognitive Synthesis\n    EmergentCognition --&gt; AdaptiveLearning: Pattern Learning\n    AdaptiveLearning --&gt; [*]: Updated Knowledge\n\n    note right of Integration\n        Neural-symbolic synergy:\n        - Pattern correlation\n        - Knowledge fusion\n        - Emergent insights\n        - Adaptive learning\n    end note</code></pre>"},{"location":"space-tokenizer/#cognitive-space-hierarchies","title":"Cognitive Space Hierarchies","text":"<p>The space system implements hierarchical cognitive organization that enables emergent attention allocation and adaptive reasoning patterns.</p> <pre><code>graph TB\n    subgraph \"Cognitive Hierarchy\"\n        A[Meta-Cognitive Space] --&gt; B[Abstract Reasoning Space]\n        B --&gt; C[Domain-Specific Spaces]\n        C --&gt; D[Operational Spaces]\n        D --&gt; E[Primitive Spaces]\n    end\n\n    subgraph \"Attention Allocation\"\n        F[Global Attention Context] --&gt; G[Focused Attention Areas]\n        G --&gt; H[Local Processing Zones]\n        H --&gt; I[Micro-Attention Units]\n    end\n\n    subgraph \"Knowledge Integration\"\n        J[Cross-Domain Knowledge] --&gt; K[Domain Knowledge]\n        K --&gt; L[Specific Knowledge]\n        L --&gt; M[Factual Knowledge]\n    end\n\n    A -.-&gt; F\n    B -.-&gt; G\n    C -.-&gt; H\n    D -.-&gt; I\n\n    A -.-&gt; J\n    B -.-&gt; K\n    C -.-&gt; L\n    D -.-&gt; M\n\n    style A fill:#e1f5fe\n    style F fill:#e8f5e8\n    style J fill:#fff3e0\n    style G fill:#fce4ec\n    style K fill:#f3e5f5</code></pre>"},{"location":"space-tokenizer/#adaptive-attention-allocation-in-spaces","title":"Adaptive Attention Allocation in Spaces","text":"<p>The space system implements adaptive attention mechanisms that enable emergent cognitive focus and resource optimization.</p> <pre><code>flowchart LR\n    A[Attention Request] --&gt; B[Context Analysis]\n    B --&gt; C[Priority Assessment]\n    C --&gt; D[Resource Allocation]\n\n    D --&gt; E[Primary Focus]\n    D --&gt; F[Secondary Areas]\n    D --&gt; G[Background Processing]\n\n    E --&gt; H[Intensive Processing]\n    F --&gt; I[Monitoring]\n    G --&gt; J[Passive Awareness]\n\n    H --&gt; K[Deep Cognitive Analysis]\n    I --&gt; L[Pattern Watching]\n    J --&gt; M[Resource Conservation]\n\n    K --&gt; N[Active Learning]\n    L --&gt; O[Opportunity Detection]\n    M --&gt; P[Efficiency Optimization]\n\n    N --&gt; Q[Knowledge Integration]\n    O --&gt; R[Focus Reallocation]\n    P --&gt; S[Resource Rebalancing]\n\n    Q --&gt; T[Emergent Understanding]\n    R --&gt; U[Adaptive Attention]\n    S --&gt; V[Optimized Processing]\n\n    T --&gt; W[Enhanced Cognition]\n    U --&gt; W\n    V --&gt; W\n\n    style A fill:#e1f5fe\n    style K fill:#e8f5e8\n    style Q fill:#fff3e0\n    style T fill:#fce4ec\n    style W fill:#f3e5f5</code></pre>"},{"location":"space-tokenizer/#token-space-cognitive-integration","title":"Token-Space Cognitive Integration","text":"<p>The integration between tokenizer and space systems enables emergent language understanding and adaptive symbolic reasoning.</p> <pre><code>sequenceDiagram\n    participant P as Parser\n    participant T as Tokenizer\n    participant S as Space\n    participant C as Cognitive Engine\n    participant A as Attention System\n\n    P-&gt;&gt;T: parse_text(input)\n    T-&gt;&gt;T: recognize_tokens()\n    T-&gt;&gt;S: query_context(token)\n    S--&gt;&gt;T: context_info\n\n    T-&gt;&gt;C: resolve_semantics(token, context)\n    C-&gt;&gt;A: allocate_attention(semantic_complexity)\n    A--&gt;&gt;C: attention_resources\n\n    C-&gt;&gt;S: semantic_query(meaning_pattern)\n    S--&gt;&gt;C: semantic_bindings\n\n    C-&gt;&gt;C: integrate_knowledge(bindings, context)\n    C-&gt;&gt;T: enhanced_token_meaning\n    T-&gt;&gt;P: structured_atom\n\n    P-&gt;&gt;S: add_atom(structured_atom)\n    S-&gt;&gt;S: update_knowledge_base()\n    S-&gt;&gt;A: report_learning_event()\n    A-&gt;&gt;A: adapt_attention_patterns()\n\n    Note over T, C: Emergent language understanding&lt;br/&gt;through cognitive integration\n    Note over S, A: Adaptive attention based on&lt;br/&gt;knowledge complexity</code></pre>"},{"location":"space-tokenizer/#emergent-cognitive-patterns","title":"Emergent Cognitive Patterns","text":"<p>The space and tokenizer systems enable emergent cognitive patterns through recursive processing and adaptive learning mechanisms.</p> <pre><code>mindmap\n  root((Cognitive Emergence))\n    Pattern Recognition\n      Symbolic Patterns\n        Logical Structures\n        Mathematical Relations\n        Linguistic Constructs\n      Neural Patterns\n        Similarity Clusters\n        Feature Correlations\n        Activation Patterns\n      Hybrid Patterns\n        Concept Mappings\n        Cross-Modal Relations\n        Emergent Abstractions\n    Adaptive Learning\n      Knowledge Acquisition\n        Experience Integration\n        Pattern Abstraction\n        Concept Formation\n      Attention Optimization\n        Focus Refinement\n        Resource Allocation\n        Efficiency Learning\n      Language Evolution\n        Vocabulary Expansion\n        Grammar Adaptation\n        Semantic Enrichment\n    Recursive Processing\n      Self-Reference\n        Meta-Cognitive Awareness\n        Self-Modification\n        Recursive Improvement\n      Hierarchical Reasoning\n        Multi-Level Analysis\n        Abstraction Layers\n        Emergent Complexity\n      Feedback Loops\n        Learning Cycles\n        Adaptation Loops\n        Evolution Spirals\n    Neural-Symbolic Synergy\n      Knowledge Fusion\n        Symbolic Logic\n        Neural Computation\n        Hybrid Reasoning\n      Representation Integration\n        Multi-Modal Encoding\n        Cross-Format Translation\n        Unified Understanding\n      Emergent Intelligence\n        Cognitive Synthesis\n        Adaptive Behavior\n        Creative Problem Solving</code></pre> <p>This space and tokenizer architecture provides the foundation for sophisticated cognitive computing through emergent patterns, adaptive attention allocation, and neural-symbolic integration that enables the system to exhibit intelligent behavior across multiple domains and processing modalities.</p>"},{"location":"reference/atoms/","title":"Atoms","text":"<p>::: hyperon.atoms</p>"},{"location":"reference/base/","title":"Base","text":"<p>::: hyperon.base</p>"},{"location":"reference/ext/","title":"Ext","text":"<p>::: hyperon.ext</p>"},{"location":"reference/runner/","title":"Runner","text":"<p>::: hyperon.runner</p>"},{"location":"reference/stdlib/","title":"Stdlib","text":"<p>::: hyperon.stdlib</p>"}]}